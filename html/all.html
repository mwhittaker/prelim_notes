<!DOCTYPE html>
<html>
<head>
  <title>Prelim Notes</title>
  <link href='../css/style.css' rel='stylesheet'>
  <meta name=viewport content="width=device-width, initial-scale=1">
</head>

<body>
  <div id=header>
    <a href="../">Prelim Notes</a>
  </div>
  <div id="container">
<h1 id="architecture-of-a-database-system">Architecture of a Database System</h1>
<ul>
<li>Process models
<ul>
<li>Process per request</li>
<li>Thread per request</li>
<li>Process pool</li>
<li>admission control: external and internal</li>
</ul></li>
<li>Parallel architecture
<ul>
<li>Shared memory
<ul>
<li>Simple to use, no scale</li>
</ul></li>
<li>Shared nothing
<ul>
<li>Data skip, full backup, chained declustering</li>
<li>Cheap and scalable; requires extra code for everything</li>
</ul></li>
<li>Shared disk
<ul>
<li>Cache coherency protocols</li>
<li>Easy to handle partial failure, no scale</li>
</ul></li>
<li>NUMA
<ul>
<li>Affinity scheduling</li>
</ul></li>
<li>DBMS threads
<ul>
<li>Have to map user threads across multiple processes</li>
</ul></li>
</ul></li>
<li>Relational query processor
<ul>
<li>Query parsing and authorization
<ul>
<li>Parse, typecheck, and authorize (or defer to runtime to share same query between multiple users)</li>
</ul></li>
<li>Query rewrite
<ul>
<li>View expansion</li>
<li>Constant arithmetic evaluation</li>
<li>Logical rewriting of predicates</li>
<li>Semantic optimization (e.g. redundant join elimination)</li>
<li>Subquery flattening</li>
</ul></li>
<li>Query optimizer
<ul>
<li>Plan space (e.g. not left deep)</li>
<li>Selectivity estimation (e.g. histograms)</li>
<li>Search algorithms (e.g. top-down cascades)</li>
<li>Parallelization (two phase optimization: optimize then distribute)</li>
<li>Auto-tuning</li>
</ul></li>
<li>Prepared stamements, cached query plans, and re-optimization (predictable performance vs self-tuning)</li>
<li>Query executor
<ul>
<li>Iterators</li>
<li>BP-tuples vs M-tuples</li>
<li>INSERT, UPDATE, DELETE halloween problem and iterator invalidation</li>
</ul></li>
<li>Access methods
<ul>
<li>Search arguments (SARGS) and benefits (indexes: duh, heap file: CPU overheads avoided)</li>
<li>Physical RIDs prohibit movement (unluess forward pointer); don't allow B+ tree primary storage; don't store physical RIDs in B+ trees; etc.</li>
</ul></li>
<li>Data warehouses
<ul>
<li>Bitmap indexes</li>
<li>Bulk loading and version data with historical queries</li>
<li>Materialized views</li>
<li>Data cubes</li>
<li>Snowflake schemas (fact tables, dimensions, multi-level star schema)</li>
<li>Column stores</li>
</ul></li>
<li>Database extensibility
<ul>
<li>Abstract data types</li>
<li>XML (custom DB vs ADT vs shredding)</li>
<li>Full-text search</li>
<li>Extensible optimizers</li>
<li>Remote data sources</li>
</ul></li>
</ul></li>
<li>Storage management
<ul>
<li>Spatial Control
<ul>
<li>Raw access (more control, less portable) vs file access (less control, more portable)</li>
<li>Diminishing difference over time</li>
</ul></li>
<li>Temporal access
<ul>
<li>Correctness: need to flush in the right order at the right time</li>
<li>Performance: DBMS does better prefetching and avoids double buffering</li>
<li>mmap helps avoid problems</li>
</ul></li>
<li>Buffer pool
<ul>
<li>Frames, page to frame map, page to disk map, metadata</li>
<li>Replacement policies better than LRU/Clock (e.g. LRU for full table scans, replacement policies that treat root B+ tree different than heap page)</li>
</ul></li>
</ul></li>
<li>Concurrency control and recovery
<ul>
<li>Locking and latching
<ul>
<li>Lock table holds locks</li>
<li>Latches are kept in memory next to thing being latched and are for internal use</li>
<li>Differences:
<ul>
<li>Latches don't do two-phase</li>
<li>Latches should nenever deadlock</li>
<li>Latches are lighter weight</li>
<li>Latches are not tracked</li>
</ul></li>
</ul></li>
<li>Locking and logging in indexes
<ul>
<li>B+ tree concurrency: crabbing or B-link trees</li>
<li>Physical structure logging: no need to undo effects invisible to txns (nested top actions)</li>
<li>Next-key locks: physical surrogates for logical properties
<ul>
<li>Lock all indexes in range of index plus one mure; when inserting, acquire lock on next tuple.</li>
</ul></li>
</ul></li>
<li>Interdependencies of transactional storage
<ul>
<li>Everything is intertwined (e.g. ARIES assumes 2PL, e.g. writing a recoverable access method is hard)</li>
</ul></li>
</ul></li>
<li>Shared components
<ul>
<li>Catalog: stored as tables, cached in memory, often optimized</li>
<li>Memory allocator
<ul>
<li>Context-based memory allocator allocates memory in labelled contexts</li>
<li>Makes memory management easier without a garbage collector</li>
<li>Can optimize allocation for certain types of operations</li>
</ul></li>
<li>Disk Management subsystems
<ul>
<li>Due to legacy, map table to multiple files or multiple disks</li>
<li>Abstractions like RAID and SAN complicate things</li>
</ul></li>
<li>Replication services
<ul>
<li>Physical replication</li>
<li>Trigger-based replication</li>
<li>Log-based replication: log shipping and database mirroring</li>
</ul></li>
<li>Administration, monitoring, and utilities
<ul>
<li>Optimizer statistics gathering</li>
<li>Physical reorganization and index construction</li>
<li>Backup/export</li>
<li>Bulk load</li>
<li>Monitoring</li>
</ul></li>
</ul></li>
</ul>
<h1 id="the-five-minute-rule-twenty-years-later">The Five Minute Rule Twenty Years Later</h1>
<ul>
<li>Derivation of five minute rule</li>
<li>Assumes flash is a cache between RAM and disk</li>
<li>2009 5 minute rules
<ul>
<li>4 KB RAM/disk = 1.5 hours</li>
<li>64 KB RAM/disk = 5 minutes</li>
<li>4 KB RAM/flash = 15 minutes</li>
<li>4 KB flash/disk = 2.5 hours</li>
</ul></li>
<li>Flash means checkpoints can be faster</li>
<li>Page size effect on B+ trees</li>
<li>Query processing has to to take flash/disk cost into account</li>
</ul>
<h1 id="a-history-and-evaluation-of-system-r">A History and Evaluation of System R</h1>
<ul>
<li>Phase 0
<ul>
<li>Single-user PL/I SQL interpreter with no concurrency or recovery</li>
<li>Planned throwaway</li>
<li>Subqueries but no joins</li>
<li>Focus on refining SQL interface</li>
<li>Catalog as relations</li>
<li>XRM storage layer with TIDs, tuples with pointers to domains, and inversions</li>
<li>Optimizer minimized number of tuples</li>
<li>Optimizer was CPU bound</li>
<li>Joins are important and optimizer should optimize common case</li>
</ul></li>
<li>Phase 1
<ul>
<li>RDS (optimizer) and RSS (storage system)</li>
<li>SQL or embedded queries + query compilation + catalog dependency tagging and recompilation</li>
<li>RSS was row-store</li>
<li>B+ trees, index scans, full table scans, tnlj, inl, smj</li>
<li>Query optimizer weighted sum of RSS calls and IO using Selinger style I/O.</li>
<li>Views stored as parse trees and used for authorization</li>
<li>Logging and shadow pages</li>
<li>Hierarchical intention locks + threw away predicate locks</li>
</ul></li>
<li>Phase 2
<ul>
<li>Evaluation</li>
<li>Added EXISTS, LIKE, prepared statements, outer joins</li>
<li>Optimizer evaluated with uniform independent data</li>
<li>Shadow pages bad locality and extra book keeping</li>
<li>RU, RC, and serializable; bad implementations and serializable mostly used</li>
</ul></li>
</ul>
<h1 id="the-postgres-next-generation-database-system">The POSTGRES Next-Generation Database System</h1>
<ul>
<li>Data management vs object management vs knowledge management</li>
<li>Query language + multiple programming languages + few concepts</li>
<li>Data models
<ul>
<li>Classes (with inheritance), instances, attributes, OIDs</li>
<li>Base classes, derived classes, versions</li>
<li>Base types, array types, compond types</li>
<li>C functions, operators, POSTQUEL functions</li>
</ul></li>
<li>POSTQUEL
<ul>
<li>Subqueries, transitive closures, inheritance, time travel</li>
</ul></li>
<li>Fast path
<ul>
<li>Sometimes easier than constructing SQL + allows for optimization tricks</li>
</ul></li>
<li>Rules
<ul>
<li>Rules for view maintenance, triggers, constraints, etc.</li>
<li>Forward chaining and backward chaining</li>
<li>Record-level and query rewrite implementations</li>
<li>Deferred vs immediate and same txn vs different txn</li>
<li>Using rules for views and versions</li>
</ul></li>
<li>Storage system
<ul>
<li>No-overwrite storage system</li>
<li>Fast recovery (no need to undo) and historical queries</li>
</ul></li>
<li>Implementation
<ul>
<li>Process per query</li>
<li>Table driven parser and optimizer</li>
<li>Dynamically loaded types, operators, and functions</li>
</ul></li>
</ul>
<h1 id="the-gamma-database-machine-project">The Gamma Database Machine Project</h1>
<ul>
<li>Software architecture
<ul>
<li>Horizontal parititioning with round robin, hash, or range</li>
<li>Catalog manager, query manager, scheduler, operator processes</li>
<li>Queries embedded in C or issued ad-hoc</li>
<li>Standard relational optimizer with distributed joins</li>
<li>Operators act as single node operators with split table attached</li>
</ul></li>
<li>Query processing algorithms
<ul>
<li>Selection trivial</li>
<li>Distributed SMJ, Grace hash, simple hash, and hybrid hash equijoins</li>
<li>Distributed hybrid hash join implementation (k disks, m procs)</li>
<li>Distributed group by (local agg then hash grouping keys)</li>
<li>Insert standard w/ movement when partitioning key changed</li>
</ul></li>
<li>Concurrency control
<ul>
<li>S, X, IS, IX, SIX file and page locks</li>
<li>Local deadlock detectors plus periodic global detector (time halved and doubled)</li>
</ul></li>
<li>Recovery
<ul>
<li>Updates create globally unique LSNs</li>
<li>Log entries sent to centralized log managers</li>
<li>Log manager sends flushed LSNs back to operators</li>
<li>Scheduler sends commit or abort to log managers</li>
<li>On abort, operators get aborted entries from log manager</li>
<li>Keep extra pages around to avoid waiting on log manager</li>
</ul></li>
<li>Fault tolerance</li>
<li>Chained vs interleaved declustering</li>
</ul>
<h1 id="access-path-selection-in-a-relational-database-management-system">Access Path Selection in a Relational Database Management System</h1>
<ul>
<li>RSS storage format
<ul>
<li>Tuples from multiple tables in the same page</li>
<li>Multiple pages form a segment</li>
<li>SARGS</li>
</ul></li>
<li>cost = page fetches + (w * rsi calls)</li>
<li>Selection factor estimation (assume everything is uniform)</li>
<li>Output size is FROM size multiplied by selection factors</li>
<li>RSI calls is FROM size multiplied by selection factors of sargable preds (assumes we look at every tuple once)</li>
<li>Costs of clustered and unclustered indexes + full table scan</li>
<li>Interesting orders</li>
<li>Query optimization algorithm</li>
<li>Interesting order equivalence classes</li>
<li>Avoid cross joins</li>
<li>Nested query evaluation and correlated subquery optimization (get unique values of correlated column)</li>
</ul>
<h1 id="query-evaluation-techniques-for-large-databases">Query Evaluation Techniques for Large Databases</h1>
<ul>
<li>Sorting
<ul>
<li>Non-uniform block sizes and forecasting</li>
<li>Dynamically adjusted block sizes to balance I/O and CPU</li>
<li>Smarter merging when the number of runs is not a multiple of the number of blocks.</li>
<li>Smaller fanout for the last merge so it doesn't consume too much memory when fed into parent</li>
</ul></li>
<li>Hashing
<ul>
<li>Merge small overflow buckets together if needed</li>
<li>Hybrid hash join: build a hash table in memory and dynamically spill things out to disk. Three options:
<ul>
<li>Write a fixed fraction of the hash table to disk</li>
<li>Bucket tuning and dynamic destaging: write lots of small buckets to disk which later get merged. Spill biggest partitions to disk.</li>
<li>Use precomputed stats</li>
</ul></li>
</ul></li>
<li>Aggregation and duplicate removal
<ul>
<li>Nested loop algorithm: for each tuple, scan output groups/relation</li>
<li>Sort approach is good for rollups</li>
</ul></li>
<li>Joins
<ul>
<li>SMJ can be performed on two indexes</li>
<li>Heap-filter merge-join: sort smaller inner relation; process replacement sorted blocks on the output relation and merge with inner.</li>
</ul></li>
<li>Division: R(A, B) and S(B)
<ul>
<li>Direct sort-based method: sort R and S. Scan over R and repeatedly scan over S. Emit tuples in R that match everything in S.</li>
<li>Direct hash-based method: Create a hash table indexing tuples s_1, ..., s_n in S. Assign n-length bitvector for each tuple in R and record which tuples match everything in S.</li>
<li>Divisor (denominator) partitioning: partition S and output tuples that match all parts of S.</li>
<li>Quotient (numerator) partitioning: partition R and output tuples that match any partition.</li>
</ul></li>
<li>Duality of hash and sort
<ul>
<li>Merging and partitioning are duals.</li>
<li>Sorting is good because it produces interesting orders; it's bad because the fanout depends on the size of the larger relation and it is algorithmically slower</li>
<li>Hashing is good because it depends on the size of the smaller relation and is algorithmically faster; bad because of skew and no interesting orders</li>
</ul></li>
<li>Execution of complex query plans
<ul>
<li>Complex plans have multiple operators open at once</li>
<li>Right deep plans can build hash tables on left base tables concurrently</li>
<li>Ingres style: repeatedly run and materialize one (or more) operator at a time; use stats for later parts of plan.</li>
</ul></li>
<li>Mechanisms for parallel query execution
<ul>
<li>Speedup vs scaleup</li>
<li>Distributed (multiple independent databases) vs parallel (one database managing multiple compute nodes)</li>
<li>Interquery vs (vertical vs bushy) inter-operator vs intra-operater</li>
<li>Bracket (all tuples sent via IPC) vs operator</li>
<li>Shared nothing vs shared disk vs shared nothing (+ hybrid)</li>
</ul></li>
<li>Parallel algorithms
<ul>
<li>Sort
<ul>
<li>Range partition across nodes, then each node sorts</li>
<li>Locally sort, then range partition and send sorted runs, nodes merge received sorted runs</li>
<li>Deadlock is possible if results are merged without buffering</li>
</ul></li>
<li>Equijoin
<ul>
<li>Partitioned symmetric hash join</li>
<li>Distributed Grace join (ala Gamma)</li>
<li>Partition the bigger relation and fully replicate the smaller</li>
<li>Lucja Kot bloom filter semijoin</li>
</ul></li>
<li>Join
<ul>
<li>Symmetric fragment and replicate (grid join)</li>
</ul></li>
</ul></li>
<li>Non-standard query processing algorithms
<ul>
<li>Nested relations: unnest and nest</li>
<li>Scientific and time series
<ul>
<li>Band join (R.a - c &lt;= S.a &lt;= R.a + c) with modified sort or hash join</li>
</ul></li>
<li>ODBMS</li>
<li>Control operators
<ul>
<li>store-and-scan (cache)</li>
<li>split (buffering + disk spill)</li>
<li>Active scheduler (arrows pointing away) vs passive scheduler (a la Click)</li>
<li>choose-plan Ingres style</li>
</ul></li>
</ul></li>
<li>Advanced techniques
<ul>
<li>Precomputation: views, join indexes, indexe</li>
<li>Compression
<ul>
<li>Index key compression and straight up compression</li>
<li>Less I/O, computation on compressed data, minimize skew</li>
</ul></li>
<li>Surrogates
<ul>
<li>Don't copy tuples, just point to them</li>
</ul></li>
<li>Bloom filters</li>
</ul></li>
</ul>
<h1 id="the-volcano-optimizer-generator-extensibility-and-efficient-search">The Volcano Optimizer Generator: Extensibility and Efficient Search</h1>
<ul>
<li>Design principles
<ul>
<li>Lots of domains modelled with algebras</li>
<li>Pattern matching is good</li>
<li>Don't use IRs</li>
<li>Compile plans rather than interpret</li>
<li>Use DP</li>
</ul></li>
<li>Inputs
<ul>
<li>Logical operators</li>
<li>Physical algorithms</li>
<li>Logical to logical transformation rules</li>
<li>Logical to physical transformation rules</li>
<li>Enforcers (algorithms that don't correspond to logical operator)</li>
<li>Abstract cost data type and cost function for algorithms</li>
<li>Logical and physical properties data type and function for extracting properties of expressions and plans</li>
<li>Applicability function saying whether a given algorithm can implement an expression with the given physical properties and the physical requirements on its inputs</li>
</ul></li>
<li>Search algorithm
<ul>
<li>Parameterized on logical expression, physical properties, and cost limit</li>
<li>3 moves: logical to logical, logical to physical, and enforcer</li>
<li>Memoize (logical expression, physical properties) -&gt; best plan</li>
<li>A couple of tricks to avoid infinite loops and to speed things up
<ul>
<li>In progress flags</li>
<li>Know when some physical properties imply others</li>
</ul></li>
</ul></li>
</ul>
<h1 id="eddies-continuously-adaptive-query-processing">Eddies: Continuously Adaptive Query Processing</h1>
<ul>
<li>Adaptability over best case performance</li>
<li>Synchronization barriers (sort-merge join)</li>
<li>Moments of symmetry (nested loop join)</li>
<li>Pre-optimization</li>
<li>Done and ready bits</li>
<li>Eddy as a wrapper around data sources and n operators</li>
<li>Each operator runs in its own thread with a message queue</li>
<li>Naive eddy: learn delay of operators by filling up message queues</li>
<li>Fast eddy: learn selectivities of operators using lottery scheduling</li>
<li>Dynamic performance and windowed lottery scheduling</li>
</ul>
<h1 id="worst-case-optimal-join-algorithms">Worst-Case Optimal Join Algorithms</h1>
<ul>
<li>What does it mean to worst case optimal?</li>
<li>What is the AGM bound?</li>
<li>What is the main idea behind the optimal join algorithm? Skew.</li>
<li>Rough structure of join: partition hypergraph and recurse and stuff.</li>
<li>What is leapfrog join?</li>
<li>Two choice algorithm</li>
<li>Why not do optimal join algorithms? Implementation complexity; cache locality; worst case isn't always best.</li>
</ul>
<h1 id="datalog-and-recursive-query-processing-sections-1-3-6">Datalog and Recursive Query Processing (Sections 1-3, 6)</h1>
<ul>
<li>Syntax</li>
<li>Three semantics: model, lfp, proof</li>
<li>Semi-postive datalog with negation and stratified datalog with negation</li>
<li>Stratified datalog with negation and aggregation</li>
<li>Naive and semi-naive evaluation</li>
<li>Query-subquery evaluation</li>
<li>Magic sets</li>
<li>Applications
<ul>
<li>Program analysis</li>
<li>Declarative networking</li>
<li>Data integration and exchange</li>
<li>Enterprise software</li>
<li>Concurrent programming</li>
</ul></li>
</ul>
<h1 id="aries-a-transaction-recovery-method-supporting-fine-granularity-locking-and-partial-rollbacks-using-write-ahead-logging">ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging</h1>
<ul>
<li>WAL protocol: every change to a data object needs a record in a log; the record gets flushed before the corresponding change to the data item. - WAL is faster that force approach because data changes are smaller than the pages being changed and the log is append-only.</li>
<li>Periodically flushing pages to disk helps speed up recovery because the recLSNs will be bigger and we don't have to go back as far.</li>
<li>Checkpoint location is stored in the <em>master record</em>.</li>
<li>end records are written for committed transactions at the end of the redo phase</li>
<li>Shadow paging:
<ul>
<li>Access pages through page table</li>
<li>Transactions copy page table, copy pages, and swap page table upon commit</li>
</ul></li>
<li>Disadvantages of shadow paging:
<ul>
<li>Fragmentation leads to bad locality</li>
<li>Lower degrees of concurrency</li>
<li>Storage overhead of shadow pages</li>
<li>Recovery can lead to deadlocks</li>
</ul></li>
<li>partial rollbacks could be used for deadlocks instead of total rollbacks</li>
<li>have to log locks and re-acquire locks for txns in the in-doubt stage of 2PC</li>
<li>parallelism: apply redo in parallel with one thread per page being redone</li>
<li>parallelism: log CLRs serially, but then redo them in parallel</li>
<li>could try partially rolling back txns and restarting them, but it would require us to log a lot of state (the locks held, the cursors into data structures, etc)</li>
<li>deferred restart: acquire locks during redo and then open up the sytem for txns when we start undo. A txn will block on a page if it needs to be undone.</li>
<li>ARIES checkpoints after recovery is done</li>
<li>Checkpointing during recovery can help avoid redundant work on subsequent crashes</li>
<li>Media recovery. Media snapshot taken as of the most recent checkpoint. To recover the media, we reload it and start redo from the most recent checkpoint.</li>
<li>Nested top action: a top action is a sequence of things we don't want to undo. We log a dummy CLR at the end of the top action to prevent undo.</li>
<li>Q: Why does aries assume 2pl?</li>
<li>Q: ARIES for T/O.</li>
</ul>
<h1 id="granularity-of-locks-and-degrees-of-consistency-in-a-shared-data-base"><a href="https://scholar.google.com/scholar?cluster=15730220590995320737">Granularity of Locks and Degrees of Consistency in a Shared Data Base</a></h1>
<ul>
<li>Granularity of Locks (Tree)
<ul>
<li>Different transactions benefit from different granularities of locks; it's a trade-off between concurrency and overhead.</li>
<li>X, S, IX, IS, SIX</li>
<li>S, X locks implicitly lock entire subtree. IS, IX locks say that some subtree might be locked in S or X mode.</li>
<li><table>
<thead>
<tr class="header">
<th></th>
<th>X</th>
<th>S</th>
<th>IX</th>
<th>IS</th>
<th>SIX</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>X</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>S</td>
<td></td>
<td>y</td>
<td></td>
<td>y</td>
<td></td>
</tr>
<tr class="odd">
<td>IX</td>
<td></td>
<td></td>
<td>y</td>
<td>y</td>
<td></td>
</tr>
<tr class="even">
<td>IS</td>
<td></td>
<td>y</td>
<td>y</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="odd">
<td>SIX</td>
<td></td>
<td></td>
<td></td>
<td>y</td>
<td></td>
</tr>
</tbody>
</table></li>
<li>Q: What's the purpose of SIX locks?</li>
<li>Q: Why not just acquire an S and an IX lock?</li>
<li>Q: What are the alternatives to SIX locks?</li>
<li>Lock nodes root to leaf; release locks leaf to root.</li>
</ul></li>
<li>Granularity of Locks (DAG)
<ul>
<li>A node is implicitly locked in shared mode if <em>one</em> ancestor is locked in shared mode.</li>
<li>A node is implicitly locked in exclusive mode if <em>all</em> ancestors are locked in exclusive mode.</li>
<li>Q: What if both implicit shared and exclusive required a majority of ancestors? Would that scheme work?</li>
<li>This locking scheme is equivalent to locking leaves of a tree/DAG with S and X locks (but is more efficient).</li>
</ul></li>
<li>Granularity of Locks (Dynamic DAGs)
<ul>
<li>Files, indexes, records, etc can be added, changed, and deleted.</li>
<li>Motivating example of an index interval lock (e.g. lock all records with 10 &lt; age &lt; 18).</li>
<li>To move an object in the DAG (e.g. move a tuple from one index interval to another) lock both the origin and destination in X mode.</li>
</ul></li>
<li>Granularity of Locks (Scheduling Requests)
<ul>
<li>Maintain a FIFO queue of lock requests.</li>
<li>Grant as many mutually compatible requests from the top of the queue as possible.</li>
<li>Whenever a lock is released, try again to grant as many mutually compatible requests as possible.</li>
</ul></li>
<li>Granularity of Locks (Conversions)
<ul>
<li>Sometimes we re-request locks or try to upgrade locks.</li>
<li>Can only grant conversion when it is compatible with other outstanding locks.</li>
<li>Can prioritize conversions over new lock requests.</li>
<li>Can lead to deadlock.</li>
</ul></li>
<li>Degrees of consistency
<ul>
<li>Degree 0: No overwriting dirtied values; short X.</li>
<li>Degree 1: No dirty writes; long X; &lt; acyclic.</li>
<li>Degree 2: No dirty reads; long X; short S; &lt;&lt; acyclic.</li>
<li>Degree 3: No non-repeatable reads; long X; long S; &lt;&lt; acyclic.</li>
<li>Can run transactions at multiple degrees, but higher degree transactions reading lower degree outputs will lead to inconsistency.</li>
<li>&lt;: W -&gt; W</li>
<li>&lt;&lt;: W -&gt; W, W -&gt; R</li>
<li>&lt;&lt;&lt;: W -&gt; W, W -&gt; R, R -&gt; W</li>
</ul></li>
<li>Degrees of consistency (Recoverability)
<ul>
<li>Recoverable: a transaction doesn't commit until all transactions from which it read commit.</li>
<li>Avoids Cascading Aborts: no dirty reads.</li>
<li>Strict: no dirty reads or writes.</li>
<li>Degree 0 is not recoverable, so recovery might lose updates.</li>
<li>Degree 1 is recoverable, but transactions could read aborted stuff.</li>
<li>Degree 2 and 3 are recoverable to a consistent state.</li>
</ul></li>
<li>Degrees of consistency (Cost)</li>
</ul>
<h1 id="concurrency-control-in-distributed-database-systems"><a href="https://scholar.google.com/scholar?cluster=5576139455848332932">Concurrency Control in Distributed Database Systems</a></h1>
<ul>
<li>Database model
<ul>
<li>Each site runs a transaction manager (TM) and database manager (DM).</li>
<li>A transaction talks to a TM which coordinates with to DMs.</li>
<li>Logical value $X$ stored in replicas $x_1, \ldots, x_n$.</li>
<li>Transactions read and write values from a private workspace. Two-phase commit is used to transfer private workspaces into the database. pre-write used for first round of 2PC; dm-write used for second round.</li>
</ul></li>
<li>Decomposition of the concurrency control problem
<ul>
<li>Schedules are represented as a log of reads and writes for each DM.</li>
<li>Conflict serializability is defined for this type of schedule.</li>
<li>Concurrency control decomposed in terms of rw/wr conflicts, ww conflicts, and the glue that binds them together.</li>
</ul></li>
<li>2PL synchronization techniques
<ul>
<li><strong>Basic 2PL</strong>: read lock on one replica, write locks on all replicas.</li>
<li><strong>Primary Copy 2PL</strong>: read lock and write lock on primary copy.</li>
<li><strong>Voting 2PL (ww only)</strong>: wait for a majority of write locks.</li>
<li><strong>Centralized 2PL</strong>: all locks sent to single DM.</li>
<li>Deadlock detection and prevention:
<ul>
<li>Wound-wait and wait-die (with care not to abort a transaction in the second phase of 2PL).</li>
<li>Conservative 2PL (aka predeclaration).</li>
<li>Centralized and hierarchical deadlock detection.</li>
<li>Phantom deadlocks caused by aborts for reasons other than deadlock.</li>
</ul></li>
</ul></li>
<li>Timestamp ordering synchronization techniques (with weird buffering)
<ul>
<li>To ensure recoverability and to cooperate with 2PC, T/O requires buffering which can be similar, but not equivalent to, acquiring locks.</li>
<li><strong>Basic T/O implementation</strong>.
<ul>
<li>Every transaction is given a timestamp, and operations are executed in timestamp order. A transaction is aborted (and restarted with a larger timestamp) if it detects an error in the timestamp ordering.</li>
<li>With private workspaces and 2PC, we cannot allow a dm-read or dm-write until the prewritten writes with lower timestamps are done. Otherwise, for example, a read could read the wrong value.</li>
<li>Buffer reads, prewrites, and writes. Reads cannot be issued until all previous pending prewrites are done. Writes cannot be issued until all previous pending reads are done.</li>
</ul></li>
<li><strong>Thomas Write Rule</strong>: If a write has an older timestamp than the thing it's writing, ignore the write.</li>
<li><strong>Multiversion T/O</strong>
<ul>
<li>Maintain list of read timestamps and list of (write timestamp, write value).</li>
<li>Reads are never rejected and read the latest version.</li>
<li>Writes are accepted only if no read occurred after it and before the next write.</li>
<li>As with basic T/O, we have to buffer reads, prewrites, and writes to ensure that a reads don't read the wrong value.</li>
</ul></li>
<li><strong>Conservative T/O</strong>
<ul>
<li>All TMs send dm-read and dm-write requests to DMs in increasing timestamp order.</li>
<li>A dm-read is not executed until the min dm-write request from all TMs is bigger; a dm-write is not executed until the min dm-read request from all TMs is bigger.</li>
<li>This is overly conservative; we serialize all operations, not just the conflicting ones.</li>
<li>We give each TM a transaction class: a read and write set. A transaction can execute at a TM only if its (predeclared) read and write sets are subsets.</li>
<li>A DM only waits for timestamps from transaction classes with intersecting write sets (for ww conflicts) or intersecting read/write sets for (rw conflicts).</li>
</ul></li>
<li>Garbage collection
<ul>
<li>Instead of storing the read and write timestamp for every object, store it for a fixed number of objects. For all other objects, conservatively assume the timestamp is the minimum of the stored objects.</li>
</ul></li>
</ul></li>
<li>CC Methods
<ul>
<li><strong>Pure 2PL Methods</strong>
<ul>
<li>The 12 pure 2PL methods are {basic, primary copy, centralized} for rw concurrency control cross producted with {basic, primary copy, centralized, voting} for ww concurrency control.</li>
<li>We have read locks, rw write locks, ww write locks, and rww write locks; rw write locks conflict with read locks; ww write locks conflict with ww write locks; rww write locks conflict with read locks, ww write locks, and rww write locks. The different combinations of 2PL methods vary in the type of locks they set.</li>
</ul></li>
<li><strong>Pure TO Methods</strong>
<ul>
<li>The 12 pure 2PL methods are {basic, multiversion, conservative} for rw concurrency control cross producted with {basic, Thomas write rule, multiversion, conservative} for ww concurrency control.</li>
<li>foo</li>
</ul></li>
<li><strong>Mixed Methods</strong>
<ul>
<li>Every item has a lock time and a txn is assigned a time larger than all lock times</li>
<li>Basic 2PL rw + TWW ww:
<ul>
<li>Reads and writes aquire read locks and ww locks</li>
<li>Data items tagged with write timestamps</li>
<li>After all prewrites, get a timestmp and check all writes</li>
<li>Writes don't block writes</li>
</ul></li>
<li>TO rw + 2PL ww
<ul>
<li>Conservatively issue all prewrites and get a timestamp</li>
<li>Reads block if a buffered prewrite exists with a lock time less than the txn's timestamp</li>
<li>Writes never block</li>
<li>Read-only queries can run at any timestamp</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h1 id="concurrency-control-performance-modeling-alternatives-and-implications"><a href="https://scholar.google.com/scholar?cluster=9784855600346107276">Concurrency Control Performance Modeling: Alternatives and Implications</a></h1>
<ul>
<li>Overview
<ul>
<li>There are a lot of different concurrency control algorithms and a lot of different (and contradictory) performance analyses. This paper aims to explain which assumptions were made to achieve each of the previous results.</li>
<li>Complete database model complete with users, physical resources, and workloads.</li>
<li>This paper looks at <em>resource assumptions</em> and transaction <em>modelling assumptions</em>.</li>
</ul></li>
<li>Concurrency Control Strategies
<ul>
<li>This paper explores three concurrency control algorithms.</li>
<li><strong>Blocking</strong>. Read and write locks are acquired. Whenever a transaction is blocked, deadlock detection is run, and the youngest transaction is aborted if there is deadlock.</li>
<li><strong>Immediate-Restart</strong>. Read and write locks are acquired. Whenever a transaction cannot acquire a lock, it is aborted and restarted after some adjustable amount of time.</li>
<li><strong>Optimistic</strong>. Transactions proceed in three phases: read, validate, write. At the beginning of the validate step, they are assigned a timestamp, and transactions are aborted if they do not conform to the timestamp-prescribed serial ordering.</li>
</ul></li>
<li>Performance Model
<ul>
<li><strong>System Model</strong>: database hardware, concurrency control algorithm, etc.</li>
<li><strong>User Model</strong>: interactive or batch, etc.</li>
<li><strong>Transaction Model</strong>: workload characteristics, etc.</li>
<li>Performance model includes a queueing system in which transactions are queued in a ready queue (waiting to become active), concurrency control queue (waiting to make concurrency control requests), object queue (waiting to access database objects), blocked queue (waiting to become unblocked), or update queue (waiting to perform a deferred update).</li>
<li>CPUs and disks are also modelled as a bunch of queues. Reads and writes are serviced FIFO with data being spread equiprobably across disks.</li>
<li>Parameters include the number of objects in the database, the min/mean/max number of objects a transaction touches, the probability a transaction writes an item that it reads, the think time, the restart delay, the maximum number of allowable active transactions, the number of cpus/disks, and the time it takes to perform a CPU action and a disk action.</li>
<li>There are a lot of other low-level things the model does not capture (e.g. context switching cost, page thrashing).</li>
</ul></li>
<li>Performance Metrics
<ul>
<li>Metrics considered include transactions per second, transaction delay, blocking ratio (the number of blocking actions per commit), restart ratio (the number of restarts per commit), the total disk utilization, and the useful disk utilization (the disk utilization for requests that are not later aborted).</li>
<li>All experiments have a common set of parameter settings which are not varied; see text for details.</li>
</ul></li>
<li>Resource-Related Assumptions
<ul>
<li><strong>Experiment 1: Infinite Resources</strong>. Given infinite resources, higher degrees of parallelism lead to higher likelihoods of transaction conflict which in turn leads to higher likelihoods of transaction abort and restart. The blocking algorithm thrashes because of these increased conflicts. The immediate-restart algorithm plateaus because the dynamic delay effectively limits the amount of parallelism. The optimistic algorithm does well because aborted transactions are immediately replaced with other transactions. Blocking also has the smallest response time variance before it starts thrashing because it restarts transactions least often.</li>
<li><strong>Experiment 2: Limited Resources</strong>. With a limited number of resources, all three algorithms thrash, but blocking performs best. Immediate-Restart performs well when the degree of parallelism is high because the adaptive restart delay limits the effective parallelism. Adding an adaptive delay to the other two algorithms makes them perform at high degrees of parallelism as well.</li>
<li><strong>Experiment 3: Multiple Resources</strong>. The blocking algorithm performs best up to about 25 resource units (i.e. 25 CPUs and 50 disks); after that, the optimistic algorithm performs best.</li>
<li><strong>Experiment 4: Interactive Workloads</strong>. When transactions spend more time &quot;thinking&quot;, the system begins to behave more like it has infinite resources and the optimistic algorithm performs best.</li>
</ul></li>
<li>Transaction Behavior Assumptions
<ul>
<li><strong>Experiment 6: Modeling Restarts</strong>. Some analyses model a transaction restart as the spawning of a completely new transaction. These fake restarts lead to higher throughput (especially for Immediate-Restart and Optimistic) because they avoid repeated transaction conflict.</li>
<li><strong>Experiment 7: Write-Lock Acquisition</strong>. Some analyses have transactions acquire read-locks and then upgrade them to write-locks. Others have transactions immediately acquire write-locks if the object will ever be written to. Upgrading locks can lead to deadlock if two transactions concurrently write to the same object. For limited parallelism, this doesn't have much of an effect. For higher degrees of parallelism, it can reduce throughput. The effect also vary with the number of resources; see text for details.</li>
</ul></li>
<li>Lessons learned
<ul>
<li>With low contention, all three concurrency control algorithms perform similarly.</li>
<li>Blocking is best for systems with medium to high resource utilization. Immediate-Restart or Optimistic is better for systems with low resource utilization.</li>
<li>Limiting the amount of parallelism can prevent thrashing.</li>
<li>Fake-restarts benefit Immediate-Restart and Optimistic, making Blocking look bad.</li>
<li>No-lock upgrade assumption favors Immediate-Restart and then Blocking.</li>
</ul></li>
<li>Conclusion
<ul>
<li>A database model needs system model, user model, and transaction model.</li>
<li>If there is medium to high utilization, then blocking algorithms prevent wasted work and improve throughput. If there is low utilization, then restart-oriented algorithms can tolerate the wasted utilization and are preferable.</li>
</ul></li>
</ul>
<h1 id="efficient-locking-for-concurrent-operations-on-b-trees">Efficient Locking for Concurrent Operations on B-trees</h1>
<ul>
<li>Example problem of B+ tree search without locks</li>
<li>B-link tree high keys and sibling pointers</li>
<li>Search algorithm: rightward walk without locks</li>
<li>Insertion algorithm:
<ul>
<li>Search down without locks and remember stack</li>
<li>Crab rightwards for leaf</li>
<li>Walk up stack and crab rightwards</li>
<li>At most 3 locks at any given point in time</li>
</ul></li>
<li>Deletion: let tree underflow</li>
</ul>
<h1 id="improved-query-performance-with-variant-indexes">Improved Query Performance with Variant Indexes</h1>
<ul>
<li>Value-List indexes
<ul>
<li>B+ tree index with RID list or bitmap at leaves</li>
<li>If there are a small number of keys, then the bitmap uses less space</li>
</ul></li>
<li>Projection indexes
<ul>
<li>A column of a table stored contiguously (like columnar storage)</li>
</ul></li>
<li>Bit-Sliced indexes
<ul>
<li>Values viewed as list of bits and stored column-by-column</li>
</ul></li>
<li>SELECT SUM(c) FROM R WHERE p
<ul>
<li>Assume bitmap fp</li>
<li>No index: full table scan, filtering out using fp</li>
<li>Value-List bitmap index: for each k, compute fk and then k * popcount(fk and fp)</li>
<li>Bit-sliced index: for each i, 2^i * popcount(fp and ci).</li>
</ul></li>
<li>SELECT * FROM R WHERE c &gt; 100 and p
<ul>
<li>Assume bitmap fp</li>
<li>No index: full table scan</li>
<li>Value-List bitmap: (OR of all bitmaps for c &gt; 10) AND fp</li>
<li>Projection index: full column scan</li>
<li>Bit-sliced index: intense bit tricks</li>
</ul></li>
<li>GROUP BY (A, B)
<ul>
<li>Value-List index: for each fa for A and fb in B, compute (fa AND fb) and use it to compute the GROUP BY</li>
<li>Projection index: Perform the GROUP BY like normal but only look at the relevant columns</li>
</ul></li>
</ul>
<h1 id="the-r-tree-an-efficient-and-robust-access-method-for-points-and-rectangles">The R*-tree: an Efficient and Robust Access Method for Points and Rectangles</h1>
<ul>
<li>R-tree
<ul>
<li>Like a B+ tree but maps bounding boxes to RIDs</li>
<li>Internal nodes contain bounding boxes of children</li>
<li>Search searches multiple paths, whichever overlaps the query</li>
<li>Insert searches down the path which requires least enlargement (ties broken by area) and potentially splits rearrange to minimize overlap</li>
<li>Delete just deletes</li>
</ul></li>
<li>Optimization goals
<ul>
<li>Minimize rectangle area</li>
<li>Minimize rectangle overlap</li>
<li>Minimize rectangle perimeter</li>
<li>Maximize storage utilization</li>
</ul></li>
<li>R-trees vary on find subchild and split
<ul>
<li>Original find subchild: choose one with minimum enlargement (break ties on area)</li>
<li>Original split: minimize overlap</li>
<li>Original split (quadratic approximation): choose two seeds which fill least area of bounding box, then repeatedly pick the other boxes that have the largest difference in area increase when assigned to both groups</li>
<li>Greene's split: choose seeds as above and then divide in half along most distant axis</li>
</ul></li>
<li>R* tree find subchild
<ul>
<li>For internal nodes pointing at internal nodes, use original find</li>
<li>For internal nodes pointing at children, choose child which increases overlap the least, breaking ties by least area increase and then least area. The overlap for a child is the sum of overlaps with all other children.</li>
</ul></li>
<li>R* tree split
<ul>
<li>For each axis, perform all splits where each split has at least m entries</li>
<li>Compute the sum of perimeters for the bounding boxes of all splits</li>
<li>Choose axis which minimizes perimeter</li>
<li>Then choose the partition which minimizes overlap, ties broken by area</li>
</ul></li>
<li>Reinsert values (in decreasing order of how far away they are from the bounding box)</li>
</ul>
<h1 id="the-log-structured-merge-tree-lsm-tree">The log-structured merge-tree (LSM-tree)</h1>
<ul>
<li>2 component LSM-tree
<ul>
<li>C0 in-memory component (some sort of tree, not necessarily a B+ tree)</li>
<li>C1 on-disk, densely-packed, contiguously-allocated B+ tree component</li>
<li>Multi-page sections of C0 and C1 are merged together and written back to C1, sort of like bulk inserting</li>
<li>Finds must read from C0 and then from C1</li>
<li>Deletions tombstoned in C0</li>
<li>Batch deletions can be deferred to when the portion of the deletes are brought into memory for merging</li>
</ul></li>
<li>Multi-component LSM-trees
<ul>
<li>Imagine a huge C1 and a small C0. The number of pages for a given range in C1 might be way bigger than the number of pages for C0. If so, we end up reading way too many pages</li>
<li>Multi-component LSM trees has a series of increasingly large trees on disk</li>
<li>There is a way to find the optimal sizes that is described in the paper, but its complicated</li>
</ul></li>
<li>Concurrency control
<ul>
<li>Must avoid conflicts that occur when
<ul>
<li>someone reads the part of a disk tree that is being merged,</li>
<li>someone searches the memory tree when it is being modified, or</li>
<li>when multiple merges on the same tree are taking place.</li>
</ul></li>
<li>Acquire read/write locks on the nodes of the disk trees</li>
<li>Do normal CC on the in-memory tree</li>
</ul></li>
<li>Recovery
<ul>
<li>Rely on normal log</li>
<li>When a checkpoint happens,
<ul>
<li>Finish all pending merges</li>
<li>Write C0 to disk</li>
<li>Flush all dirty disk components</li>
<li>Record the LSN to restart from, the cursors of all components, and some other info.</li>
</ul></li>
</ul></li>
</ul>
<h1 id="c-store-a-column-oriented-dbms"><a href="https://scholar.google.com/scholar?cluster=12924804892742402591">C-Store: A Column-Oriented DBMS</a></h1>
<ul>
<li>Introduction
<ul>
<li>Row-stores are write optimized; a write takes a single disk seek.</li>
<li>Column-stores are read optimized; they exploit tricks like only reading relevant columns, compressing columns (trading CPU for memory and disk bandwidth), redundant storage with different sort orders, dense-packing indexes, vector processing, etc.</li>
</ul></li>
<li>Data model
<ul>
<li>Relations are stored as a collections of <strong>projections</strong>. A projection anchored on a relation $R$ is a subset of the columns in some sorted order (e.g. $R(a, b)$ sorted on $b$).</li>
<li>Projections are stored column-by-column.</li>
<li>Projections are horizontally partitioned across a cluster on their sort key.</li>
<li>Each entry in a segment is assigned a segment-unique storage key (stored explicitly in WS but not RS).</li>
<li>A join index matches up one projection $P$ segment-by-segment to another projection $Q$. Logically each segment $P_i$ in $P$ with $n$ rows has an $n$-row join index table with rows of the form (segment id, storage key). The $i$th entry in the join index table locates the corresponding row in $Q$.</li>
<li>To reconstruct an entire table in some sort order from a set of projections, we need join indexes to map a covering set of projections to that sort order.</li>
<li>Join indexes are very expensive to maintain. Modifying a projection requires modifying all the join indexes pointing into or out of it. This is why they are good for read-only workloads.</li>
<li>Horizontally partitioned projections with redundancy can be used to achieve fault tolerance.</li>
</ul></li>
<li>RS
<ul>
<li>The storage keys of tuples in RS are implicit (i.e.&#160;the $i$th tuple has storage key $i$).</li>
<li>Columns in RS are compressed based on their sort order and based on their number of distinct values.</li>
<li><strong>Self-ordered, few values</strong>: We store a dense B+ tree mapping <code>v</code> to <code>(v, offset, count)</code>.</li>
<li><strong>Foreign-ordered, few values</strong>: We store a bitmap per unique value <code>v</code>. We also store a B+ tree mapping index to value.</li>
<li><strong>Self-ordered, many values</strong>: We store a block-based delta encoding with a B+ tree mapping index to block.</li>
<li><strong>Foreign-ordered, many values</strong>: We store the projection uncompressed, but still with a B+ tree mapping index to value.</li>
</ul></li>
<li>WS
<ul>
<li>WS uses the same storage format as RS, except that projections are not compressed and that storage keys are stored explicitly.</li>
<li>WS projections are horizontally range partitioned in the same way as the RS, so that RS segments can be colocated with WS segments.</li>
<li>When a tuple is inserted, it is assigned a storage key that is larger than all current WS storage keys.</li>
<li>We use two B+ trees. (1) We store columns as (value, storage key) with a B+ tree on the storage key. (2) We store a B+ tree mapping sort key to storage key.</li>
<li>Join indexes are also partitioned to be colocated with their WS and RS counterparts.</li>
</ul></li>
<li>Updates
<ul>
<li>Tuple are inserted into WS. To avoid requiring synchronization for assigned storage keys, nodes use a unique id plus a local counter to assign storage keys. The local counter is initialized to be bigger than the largest storage key in RS to ensure WS storage keys are consistent with RS storage keys.</li>
<li>C-Store provides snapshot isolation for read-only transactions; a.k.a. read-only transactions read at some point in the past.</li>
<li>Every tuple in WS is annotated with an insertion, and every tuple in RS and WS is annotated with a deletion time. A snapshot read at a certain time $t$ can only read tuples inserted before $t$ and deleted after $t$.</li>
<li>C-Store maintains a <strong>low and high watermark</strong>. Snapshot reads can be issued for any time between the low and high watermark.</li>
<li>Every tuple in RS is guaranteed to be inserted before the low water mark. But note that some WS tuples may also be inserted before the low watermark. Some tuples in the RS may also have been deleted before the low watermark.</li>
<li>The timestamps are coarse-grained epochs where each epoch lasts a couple of seconds.</li>
<li>To increase epochs and the high water mark, a centralized timestamp authority decides to increment the epoch and does so after a round of communication ensuring all nodes have finished the last epoch. The latest completed epoch is the high water mark.</li>
</ul></li>
<li>Transactions
<ul>
<li>Read-write transactions use distributed two-phase locking with deadlock detection based on timeouts.</li>
<li>C-Store can commit a transaction without 2PC by not sending prepare messages or soliciting votes. The paper is short on details on how they pull this off.</li>
</ul></li>
<li>Recovery
<ul>
<li>Recovering nodes gather information from other projections on other nodes.</li>
<li>This is a really complicated part of the paper.</li>
</ul></li>
<li>Tuple mover
<ul>
<li>Tuple mover moves tuples from WS into RS deleting the tuples in WS and RS that were deleted before the low watermark. Join indexes are also updated.</li>
<li>The timestamp authority periodically increments low watermark.</li>
</ul></li>
<li>Query optimizer
<ul>
<li>C-Store has operators for e.g.&#160;decompressing, bitmap logic, applying bitmaps, concatenating projections, etc. The operators return 64 KB chunks of tuples.</li>
<li>The optimizer has to take into account the cost of decompressing and which projections to use. A lot of this is left to future work.</li>
</ul></li>
<li>Questions
<ul>
<li>Q: The paper claims that row-storage &amp; B+ trees are write-optimized while other data structures like bit map indexes are read optimized. Why?</li>
<li>A: A row store will only store one copy of a relation and use a bunch of B+ tree indexes to access it. Because we only store a relation once, it can only be clustered on one set of attributes. If we store multiple projects with different sort orders, we can traverse relations in different orders efficiently. Because the workload is read-only, maintaing these redundant copies is not expensive.</li>
<li>Q: Both C-Store and Spanner use multiversioning and timestamps to allow lock-free snapshot isolation read-only transactions. Why is C-Store&#8217;s implementation so much simpler?</li>
<li>A: ??? C-Store probably isn&#8217;t linearizable. Maybe not even serializable.</li>
</ul></li>
</ul>
<h1 id="hekaton-sql-servers-memory-optimized-oltp-engine"><a href="https://scholar.google.com/scholar?cluster=14161764654889427045">Hekaton: SQL Server&#8217;s Memory-Optimized OLTP Engine</a></h1>
<ul>
<li>Overview
<ul>
<li>Hekaton is an embedded OLTP engine inside of Microsoft SQL Server.</li>
<li>Hekaton uses tricks like storing everything in memory, compiled stored procedures, latch-free data structures, and fancy optimistic multiversion concurrency control to efficiently implement transactions.</li>
</ul></li>
<li>Guiding principles
<ul>
<li>Optimized indexes for main memory (and don&#8217;t log index operations; recover them completely from scratch upon recovery).</li>
<li>Eliminate latches and locks.</li>
<li>Compile requests to native code.</li>
<li>Don&#8217;t partition data.</li>
</ul></li>
<li>Storage and indexing
<ul>
<li>Hekaton relations are stored row-by-row where each tuple is of the form (logical begin timestamp, logical end timestamps, index links, data).</li>
<li>Latch-free hash indexes and Bw-tree indexes point into the relations, and the links within the tuples connect tuples that fall within the same bucket.</li>
<li>If a read is performed at time $t$, then only tuples with a begin time before $t$ and an end time after $t$ are read. Reads at time $t$ read all versions within a bucket but only return the versions as of time $t$.</li>
<li>When a transaction deletes a tuple, it places its transaction id in the end timestamp field (and later replaces it with its commit timestamp).</li>
<li>When a transaction inserts a tuple, it places its transaction id in begin timestamp field (and later replaces it with its commit timestamp).</li>
</ul></li>
<li>Query compilation
<ul>
<li>Stored procedures are optimized into mixed abstract syntax trees (MAT) and then into pure imperative trees (PIT) and then into C code.</li>
<li><code>CREATE TABLE</code> commands are compiled into a set of procedures for manipulating records in the table.</li>
<li>Query plans are compiled to C code that doesn&#8217;t use function calls; instead, it uses gotos and labels. The paper argues that this kind of code is smaller and faster. Complicated code for things like sorting is linked in and called via a function.</li>
<li>Compiled queries have some technical restrictions and limitations ( e.g.&#160;if a table&#8217;s schema is changed, stored procedures which operate on the table must be dropped).</li>
<li>Microsoft SQL Server also supports fully featured interpreted queries to overcome these restrictions.</li>
</ul></li>
<li>Transaction management
<ul>
<li>Hekaton uses an optimistic multiversion concurrency control scheme for read uncommited, snapshot isolation, repeatable read, and serializable transactions.</li>
<li>Every transaction is given a start timestamp at start and a commit timestamp at end.</li>
<li>All reads are issued as of the start timestamp.</li>
<li>Transactions first perform their reads and then perform their writes. Upon trying to commit, the transaction has to satisfy a couple of properties:
<ul>
<li>All the current read versions are the same as the ones read.</li>
<li>All the scans can be repeated.</li>
</ul></li>
<li>Repeatable read only requires the first test; snapshot isolation and read committed doesn&#8217;t require either.</li>
<li>These tests are checked with stored read sets and phantom sets.</li>
<li>Transactions record commit dependencies and wait for all commit dependencies are cleared before committing. Cascading aborts are possible. Reads are not returned to the user until all pending commit dependencies are cleared.</li>
<li>Transactions maintain a write set to replace their transaction ids with their commit timestamps in the tuples that they wrote.</li>
<li><em>Note</em> that while both C-Store/Spanner and Hekaton use timestamps, Hekaton is not allowing historical reads. Once a version is obsolete, it can be garbage collected.</li>
</ul></li>
<li>Durability and Recovery
<ul>
<li>Principles
<ul>
<li>Log instead of random access. Also, batch.</li>
<li>Push work to recovery time.</li>
<li>Do recovery in parallel.</li>
</ul></li>
<li>Hekaton stores <strong>log streams</strong> and <strong>checkpoint streams</strong> (in the form of <strong>data streams</strong> and <strong>delta streams</strong>).</li>
<li>At commit, all the insertions and deletions of a transaction (actually multiple are batched) are appended at once to stable storage in the log stream.</li>
<li>Periodically, the tail of the log stream are pushed into new or existing data and delta streams.</li>
<li>On recovery, the data and delta streams are replayed in parallel (with the delta streams filtering out the data streams) and then the tail of the log is played back.</li>
<li>Index operations not logged; all indexes are rebuilt on recovery.</li>
<li>Periodically, data and delta stream pairs are merged to collapse multiple data streams with very few remaining tuples.</li>
</ul></li>
<li>Garbage Collection
<ul>
<li>A tuple is garbage if it was deleted at a time before all pending transaction timestamps (or if it was written by a rolled back transaction).</li>
<li>Online: whenever a transaction scans the entries of an index, it can unlink old values. This helps keep hot indexes clean.</li>
<li>Offline: transaction processing nodes alternate between GC and processing to tidy up the dusty corners.</li>
</ul></li>
<li>Questions
<ul>
<li>Q: How can Hekaton&#8217;s transaction validation happen without taking any locks?</li>
<li>A: ???</li>
<li>Q: How exactly does Hekaton unlinking work during GC?</li>
<li>A: A bit unclear exactly what&#8217;s going on with unlinking.</li>
</ul></li>
</ul>
<h1 id="calvin-fast-distributed-transactions-for-partitioned-database-systems">Calvin: Fast Distributed Transactions for Partitioned Database Systems</h1>
<ul>
<li>Overview
<ul>
<li>The time during which a transaction holds its locks is known as its <strong>contention footprint</strong>. In a traditional distributed transactional database, a transaction has to run some distributed commit protocol like 2PC which greatly increases the contention footprint.</li>
<li>Calvin is a database which will reduce the contention footprint of transactions.</li>
</ul></li>
<li>Deterministic Databases
<ul>
<li>If we want to replicate a database, &#8220;synchronously replicating every database state change would have far too high of an overhead to be feasible.&#8221;</li>
<li>To avoid this overhead, a <strong>deterministic database</strong> instead replicates transaction requests annotated with a pre-agreed upon commit order and ensures that transactions commit in this order.</li>
<li>Deterministic databases avoid the need for aborting on <strong>non-deterministic failure</strong>, and <strong>deterministic failures</strong> can be handled in one phase instead of two. Every replica simply waits for a commit or abort message from every other node.</li>
</ul></li>
<li>System Architecture
<ul>
<li>Calvin is divided into three layers:
<ul>
<li><strong>Sequencing layer</strong>. This layer is responsible for globally ordering all transactions submitted to Calvin.</li>
<li><strong>Scheduling layer</strong>. This layer is responsible for executing transactions in the pre-agreed upon global order.</li>
<li><strong>Storage layer</strong>. This layer is responsible for storing data. Because the storage layer is a separate module, Calvin is not able to do things like run the physical parts of ARIES for recovery. Instead, recovery has to be purely logical.</li>
</ul></li>
<li>Sequencing layer.
<ul>
<li>Time is divided into 10 ms epochs. A sequencing node sequences and batches transaction requests into these epochs and then sends them to every scheduler in its replica.</li>
<li>The scheduler round robin shuffles the sequences from every sequencing node in its replica.</li>
<li>Across replicas, sequencing nodes replicate sequences either asynchronously or synchronously.</li>
<li>The asynchronous replication is like a primary-backup schema. Handling recovery here is complicated and not really described fully in the paper.</li>
<li>The synchronous replication uses Paxos.</li>
</ul></li>
<li>Scheduling layer.
<ul>
<li>Locking is done using 2PC with the requirements that locks are obtained in the pre-agreed upon global order.</li>
<li>Transactions are written in C++, and read and write sets have to be set upfront.</li>
<li>It is not stated in the paper explicitly, but it seems like the transactions themselves must be stored procedures and sent to the system. There doesn&#8217;t seem to be any logic to interactively relay information back to the client.</li>
<li>Schedulers broadcast all reads to all nodes participating in the transaction, and then all nodes execute the transaction.</li>
<li>Some transactions don&#8217;t know their read and write sets upfront, so they run <strong>optimistic lock location prediction</strong> (OLLP). It runs a query with loose consistency to guess the read and write sets and then submits it. If the read set changed, then the transaction is aborted.</li>
</ul></li>
</ul></li>
<li>Calvin with Disk-Based Storage
<ul>
<li>Even though Calvin executes transactions in a pre-agreed upon ordering, the disk fetches performed by transactions do not need to be run in this order.</li>
<li>Calvin can optimistically prefetch the data that will be read by a transaction before it has acquired its locks.</li>
<li>Sequencers can also delay ordering a transaction until its resources will likely not already be locked.</li>
</ul></li>
<li>Checkpointing
<ul>
<li>Calvin can do a synchronous checkpoint in which an entire replica is frozen.</li>
<li>Calvin can do a zig-zag checkpoint in which all writes after a point in the global log are written elsewhere.</li>
<li>If the storage system supports multiversioning, then Calvin can perform a checkpoint using old versions of data.</li>
</ul></li>
<li>Questions
<ul>
<li>Q: Why is the storage layer so highly coupled with so many other layers?</li>
<li>A: E.g. recovery logging cannot be physical, has to be logical. E.g. cannot do index range locks.</li>
<li>Q: If the sequencers are ordering everything, running Paxos, delaying things for optimal disk access, is the contention footprint really gone? Or has it just moved into the sequencer instead of the scheduler?</li>
<li>A: ???</li>
</ul></li>
</ul>
<h1 id="spanner-googles-globally-distributed-database">Spanner: Google&#8217;s Globally-Distributed Database</h1>
<ul>
<li>Architecture
<ul>
<li>Spanner deployments are called <strong>universes</strong>.</li>
<li>The unit of physical isolation and administration is the <strong>zone</strong>. There may be multiple zones per datacenter.</li>
<li>Each zone has a <strong>zonemaster</strong>, some <strong>location proxies</strong>, and a lot of <strong>spanservers</strong>. The zonemaster assigns data to spanservers, and the spanservers serve data to clients.</li>
<li>There is also a single <strong>universemaster</strong> (for administration) and <strong>placement driver</strong> (for moving data around).</li>
</ul></li>
<li>Example
<ul>
<li><p>Imagine we have the following relation, divided into eight directories.</p>
<pre><code>  +----+
  | D1 |
  | D2 |
  | D3 |
  | D4 |
  | D5 |
  | D6 |
  | D7 |
  | D8 |
  +----+</code></pre></li>
<li><p>We bundle the directories into tablets:</p>
<pre><code>  T1: D1, D6
  T2: D3, D5
  T3: D2, D8
  T4: D4, D7</code></pre></li>
<li><p>Each tablet is then replicated, say three ways.</p>
<pre><code>  R1: [D1, D6]  R4: [D3, D5]  R7: [D2, D8]  R10: [D4, D7]
  R2: [D1, D6]  R5: [D3, D5]  R8: [D2, D8]  R11: [D4, D7]
  R3: [D1, D6]  R6: [D3, D5]  R9: [D2, D8]  R12: [D4, D7]</code></pre></li>
<li><p>Imagine we have five zones, with two servers per zone. We can split up the tablet replicas like this (stars signify the master):</p>
<pre><code>  Zone 1:  M1: [R1],       M2: [R10*]
  Zone 2:  M3: [R7],       M4: [R4*]
  Zone 3:  M5: [R8],       M6: [R5, R11]
  Zone 4:  M7: [R9*, R12], M8: [R2*]
  Zone 5:  M9: [R6],       M10: [R3]</code></pre></li>
</ul></li>
<li>Spanserver Architecture
<ul>
<li>Each spanserver maintains 100-100 <strong>tablets</strong>. Each tablet is a <code>(key: string, value: timestamp) -&gt; string</code> mapping stored as roughly an LSM-tree on Colossus. Note that a Spanner tablet is similar to but not quite the same as a Bigtable tablet.</li>
<li>Each tablet is replicated across a set of spanservers, and these spanservers form a Paxos group. One Paxos group is run per tablet, and the metadata and log are both stored in the tablet itself. The Paxos instance also uses 10-second leader leases. Writes to the tablet must go through Paxos. Reads can be read directly from a tablet so long as it is sufficiently up to date.</li>
<li>The leader of a Paxos group maintains a <strong>lock table</strong> for concurrency control. This lock table is not replicated via Paxos.</li>
<li><p>The leader of a Paxos group also has a <strong>transaction manager</strong> for two-phase commit. The leader is called a <strong>participant leader</strong>, the other members are <strong>participant slaves</strong>. One Paxos group is designated as the coordinator. The leader of this group is called the <strong>coordinator leader</strong>; the rest are <strong>coordinator slaves</strong>.</p>
<pre><code>Paxos Group 1 (participant)
+------------------------------------------------------------------+
| [participant leader]&lt;-&gt;[participant slave]&lt;-&gt;[participant slave] |
+------------------------------------------------------------------+
   ^
   |
   v
Paxos Group 2 (participant)
+------------------------------------------------------------------+
| [participant leader]&lt;-&gt;[participant slave]&lt;-&gt;[participant slave] |
+------------------------------------------------------------------+
   ^
   |
   v
Paxos Group 3 (coordinator)
+------------------------------------------------------------------+
| [coorindator leader]&lt;-&gt;[coorindator slave]&lt;-&gt;[coorindator slave] |
+------------------------------------------------------------------+</code></pre></li>
</ul></li>
<li>Directories
<ul>
<li>A <strong>directory</strong> is a contiguous region of the key-space with a common prefix.</li>
<li>Directories are the unit of data movement and the smallest unit for which an application can specify replication configuration (e.g.&#160;replicate this directory 5 times in Europe)</li>
<li>A tablet can contain many directories.</li>
</ul></li>
<li>Data Model
<ul>
<li>Spanner uses a hierarchical semi-relational data model; really it&#8217;s the relational model with a couple of small caveats.</li>
<li>All relations must have a primary key.</li>
<li>Relations can be nested within parents with child primary keys being prefixed by their parent primary keys.</li>
<li>These hierarchical relations are stored with rows interleaved and within the same directory.</li>
</ul></li>
<li>TrueTime
<ul>
<li>The TrueTime API has a call <code>TT.now()</code> which returns an interval of time that is guaranteed to contain the true time.</li>
</ul></li>
<li>Concurrency Control
<ul>
<li>Spanner offers four types of transactions linearizable read-write transactions, snapshot (read-only) transactions, snapshot reads at a user specified timestamp, and snapshot reads with a user specified staleness bound.</li>
</ul></li>
<li>Read-Write Transactions
<ul>
<li>Notes and Invariants
<ul>
<li>Unlike with a traditional RDBMS, clients will perform all reads and buffer all writes, and then commit all the writes at once. This means that a transaction won&#8217;t read its own write by default.</li>
<li><strong>Invariant</strong>: Paxos leader leases are disjoint.</li>
<li><strong>Invariant:</strong> Writes within a Paxos group are assigned monotonically increasing timestamps (even across leaders).</li>
<li><strong>Invariant:</strong> Every write in a transaction is tagged with the same timestamp (the timestamp of the commit).</li>
<li><strong>Invariant:</strong> Other transactions cannot read a write at a timestamp until that time has passed.</li>
<li><strong>Invariant:</strong> If a transaction commits at time $s_i$, then Spanner ensures clients cannot see the effects of the transaction until after $s_i$ has passed.</li>
</ul></li>
<li>Procedure
<ul>
<li>A client first sends reads to the appropriate Paxos leaders. The leaders maintain a lock table and acquire read locks. Leaders use wound-wait to avoid deadlock (without need for coordination).</li>
<li>After a client has performed all of its reads and buffered all of its writes, it begins two-phase commit. The client chooses a coordinator group and sends a prepare message and the identity of the coordinator group to all groups.</li>
<li>Groups aquire write locks and stage the writes. They then log a prepare message to the log with a timestamp higher than previous log entries. They relay their vote to the coordinator leader along with the prepare timestamp they choose.</li>
<li>The coordinator leader either aborts or chooses a commit timestamp greater than the prepare of all other groups and greater than the <code>latest</code> time produced by a <code>TT.now()</code> call made when the prepare request was received.</li>
<li>After the commit timestamp has passed, the coordinator leader replies to the client and also sends the commit timestamp to all other Paxos groups. At this point, the Paxos group apply their staged writes and release their locks.</li>
</ul></li>
</ul></li>
<li>Serving Reads at a Timestamp
<ul>
<li>Say we want to perform a read at timestamp $t$. Servers maintain a low watermark $t_{safe}$ with the guarantee that no more writes will happen before $t_{safe}$. A client can perform a read if $t \leq t_{safe}$.</li>
<li>So how do we compute $t_{safe}$? It is $\min(t_{safe}^{Paxos}, t_{safe}^{TM})$. Here, $t_{safe}^{Paxos}$ is the highest <em>applied</em> Paxos write. $t_{safe}^{TM}$ is ignored if there are no pending transactions. If there are pending transactions, $t_{safe}^{TM}$ is the smallest pending timestamp. This ensures that transactions do not read pending writes.</li>
<li>One confusing thing is why we need $t_{safe}^{TM}$. A spanserver does not associate a timestamp with a write until it has already committed, so pending writes should not even show up.</li>
</ul></li>
<li>Snapshot Read Transactions
<ul>
<li>Choose a time greater than <code>latest</code> from <code>TT.now()</code> and read at that timestamp.</li>
</ul></li>
<li>Refinements
<ul>
<li>The timestamps chosen for a snapshot read can be pushed backwards to the time of the most recent committed transaction. Alternatively, the transaction can choose a better one by looking at which writes conflict.</li>
<li>$t_{safe}^{Paxos}$ can be advanced in clever ways.</li>
</ul></li>
<li>Paxos Leases
<ul>
<li>TODO</li>
</ul></li>
<li>Schema change transactions
<ul>
<li>TODO</li>
</ul></li>
<li>Questions
<ul>
<li>Q: If spanservers store their tablets in Colossus (which is replicated), why do they need to run Paxos at all?</li>
<li>A: ??? Maybe Colossus doesn&#8217;t guarantee strong consistency in the face of concurrent writes.</li>
<li>Q: The paper says transaction reads do <em>NOT</em> read their own writes. How can this be true while still ensuring serializability?</li>
<li>A: ??? I guess you have to buffer your own writes?</li>
</ul></li>
</ul>
<h1 id="building-efficient-query-engines-in-a-high-level-language"><a href="https://scholar.google.com/scholar?cluster=11118963220228843116">Building Efficient Query Engines in a High-Level Language</a></h1>
<ul>
<li>Overview
<ul>
<li>Template based query compilation (where queries are directly translated operator-by-operator to stringified low-level code) misses a lot of optimization opportunities.</li>
<li>LegoBase instead compiles queries in an actually smart way.</li>
</ul></li>
<li>Architecture
<ul>
<li>LegoBase compiles queries using a standard query optimizer to get a query plan. It then converts the query plan to LegoBase&#8217;s Scala/LMS implementation of the plan. It then performs a series of optimizations before eventually compiling down to C.</li>
<li>Scala to C: straightforward except Scala libraries mapped to GLib and Scala code must manually mark allocations and deletions because C doesn&#8217;t have a garbage collector</li>
</ul></li>
<li>Optimizations
<ul>
<li>Legobase can convert Volcano style pull operators into push style query operators. The transformation involves swapping callers and callees.</li>
<li>LegoBase can eliminate the redundant materializations across multiple operators using simple pattern matching on the Scala operators.</li>
<li>LegoBase can perform data structure specialization (e.g.&#160;hash table to array).</li>
<li>LegoBase can change the data layout of a relation (e.g.&#160;between row and column oriented).</li>
</ul></li>
<li>Questions
<ul>
<li>Q: Give a concrete example of a query which builds redundant hashtables and how we can avoid creating them.</li>
<li>A: ???</li>
</ul></li>
</ul>
<h1 id="transaction-management-in-the-r-distributed-database-management-system">Transaction Management in the R* Distributed Database Management System</h1>
<ul>
<li>What's the point of 2PC
<ul>
<li>Imagine coordinator sends commit to everyone; one node commits, the other crashes and loses all of its data because it didn't force write anything</li>
</ul></li>
<li>Vanilla 2PC
<ul>
<li>coordinator force-write commits and aborts</li>
<li>Subordinate force-writes prepares, commits, and aborts</li>
</ul></li>
<li>Hierarchical 2PC</li>
<li>2PC with presumed abort
<ul>
<li>The second we know it's an abort, forget about the transaction</li>
<li>Coordinate force-writes commits</li>
<li>Subordinate force-write prepare and commit</li>
<li>Only commits are acked</li>
</ul></li>
<li>2PC with presumed commit
<ul>
<li>Coordinate force-writes collecting and commit</li>
<li>Subordinate force writes prepare and abort</li>
<li>Aborts must be acked</li>
<li>Coordinator crash between collecting and commit, assume abort</li>
</ul></li>
</ul>
<h1 id="generalized-isolation-level-definitions">Generalized Isolation Level Definitions</h1>
<ul>
<li>Degrees of consistency
<ul>
<li>English definitions</li>
<li>Degree 1: read uncommitted, long write locks, no read locks</li>
<li>Degree 2: read committed, long write locks, short read locks</li>
<li>Degree 3: serializable, long write locks, long read (phantom) locks</li>
</ul></li>
<li>ANSI SQL Standard
<ul>
<li>English definitions</li>
<li>All wrong</li>
</ul></li>
<li>Critique of ANSI SQL
<ul>
<li>Semi-formal definitions</li>
<li>P0: dirty write</li>
<li>P1: dirty read</li>
<li>P2: non-repeatable read</li>
<li>P3: phantom</li>
<li>RU: No P0</li>
<li>RC: No P0, P1</li>
<li>RR: No P0, P1, P2</li>
<li>Serializable: No P0, P1, P2, P3</li>
</ul></li>
<li>Adya
<ul>
<li>No multi-versioning</li>
<li>Prevents some schedules which are conflict serializable</li>
<li>DB model
<ul>
<li>Read and write multiple versions</li>
<li>Predicates return version set</li>
</ul></li>
<li>Dependency graphs
<ul>
<li>Read dependencies (wr) read version of someone else or read a predicate which was changed by someone else</li>
<li>Write dependencies (ww) wrote next version</li>
<li>Anti-dependency (rw) install next version or change predicate of something read by previous predicate</li>
</ul></li>
<li>G0: write cycle</li>
<li>G1: read-write cycles, aborted reads, intermediate reads</li>
<li>G2: any cycle</li>
<li>Pl-1 (RU): no G0</li>
<li>Pl-2 (RC): no G0/G1</li>
<li>Pl-3 (S): no G0/G1/G2</li>
</ul></li>
</ul>
<h1 id="managing-update-conflicts-in-bayou-a-weakly-connected-replicated-storage-system">Managing update conflicts in Bayou, a weakly connected replicated storage system</h1>
<ul>
<li>Eventually consistent gossip with user-defined write conflict resolution</li>
<li>Meeting room example</li>
<li>Dependency check SQL query + expected results</li>
<li>Merge procedure rewrites query or logs errors on failure</li>
<li>Writes timestamped and gossiped and executed in order immediately (might have to undo writes)</li>
<li>Committed writes ordered before tentative writes</li>
<li>Could commit once timestamp is greater than low watermark for all servers</li>
<li>Bayou uses single master to decide commit</li>
<li>Storage system
<ul>
<li>Write log of committed and tentative writes with garbage collection and care not to reintroduce write via old gossip</li>
<li>Tuple store with bits for tentative or committed</li>
<li>Undo log to undo stuff</li>
</ul></li>
</ul>
<h1 id="dynamo-amazons-highly-available-key-value-store">Dynamo: Amazon's highly available key-value store</h1>
<ul>
<li>Availability over performance</li>
<li>System interface
<ul>
<li>Key-value store with blob values</li>
<li>get(key) -&gt; (value list, context)</li>
<li>put(key, context, value)</li>
</ul></li>
<li>Partitioning
<ul>
<li>Consistent partitioning with virtual nodes</li>
</ul></li>
<li>Replication
<ul>
<li>Replicate to (at most) N neighboring (non-virtual) nodes in the ring</li>
</ul></li>
<li>Data versioning
<ul>
<li>Every data item is annotated with a vector clock</li>
<li>Writes and reads use W and R quorum style, so concurrent puts might conflict</li>
<li>Users have to reconcile concurrent writes</li>
<li>Shopping cart: concurrent delete and add, deleted item is re-inserted</li>
</ul></li>
<li>Failures
<ul>
<li>Sloppy quorum: if node is not free, write to one outside the preference list</li>
</ul></li>
<li>Preventing divergence
<ul>
<li>Nodes use gossip and Merkel trees to ensure they don't diverge</li>
</ul></li>
<li>Membership
<ul>
<li>Administrator has to manually add nodes</li>
<li>Gossip membership so everyone knows the ring assignments</li>
</ul></li>
<li>Implementation:
<ul>
<li>Java SEDA implementation</li>
</ul></li>
</ul>
<h1 id="cap-twelve-years-later-how-the-rules-have-changed">CAP Twelve Years Later: How the &quot;Rules&quot; Have Changed</h1>
<ul>
<li>CAP isn't hard and fast:
<ul>
<li>When no partition, you can have it all</li>
<li>CA has multiple granularities</li>
</ul></li>
<li>Partition as patience limit on communication loss</li>
<li>Partition management: detect partition, enter special mode, recover later</li>
<li>Make the easy to recover operations inconsistent</li>
<li>Recover w/ user intervention, automatic like CRDTs, and sometimes compensations</li>
</ul>
<h1 id="consistency-analysis-in-bloom-a-calm-and-collected-approach">Consistency Analysis in Bloom: a CALM and Collected Approach</h1>
<ul>
<li>CALM</li>
<li>Bloom</li>
</ul>
<h1 id="parallel-database-systems-the-future-of-high-performance-database-processing">Parallel Database Systems: The Future of High Performance Database Processing</h1>
<ul>
<li>Pipeline vs partition parallelism</li>
<li>Speedup vs scaleup</li>
<li>Overheads to perfect scalability
<ul>
<li>Startup</li>
<li>Interference</li>
<li>Skew</li>
</ul></li>
<li>Shared nothing, shared disk, shared nothing</li>
<li>Limitations to pipeline parallelism in SQL
<ul>
<li>Query plans not that deep</li>
<li>Some operators (e.g. aggregates) cannot be pipelined</li>
<li>Some operators take way longer than others</li>
</ul></li>
<li>Round robin, range, and hash partitioning</li>
</ul>
<h1 id="encapsulation-of-parallelism-in-the-volcano-query-processing-system">Encapsulation of Parallelism in the Volcano Query Processing System</h1>
<ul>
<li>Limitations of bracket model (forced IPC overhead)</li>
<li>Pipeline parallelism
<ul>
<li>Fork child, communicate through shared memory, semaphore for flow control</li>
</ul></li>
<li>Bushy parallelism: pipeline parallelism on siblings</li>
<li>Intra-operator parallelism
<ul>
<li>Fork master producer which forks multiple producers</li>
<li>All producers share same port</li>
<li>Round robin, hash, or range partition data upwards</li>
<li>Count end-of-stream to know when done</li>
<li>Less obvious how to implement</li>
</ul></li>
<li>Merge can sit on top of sorted children</li>
</ul>
<h1 id="mapreduce-simplified-data-processing-on-large-clusters">MapReduce: simplified data processing on large clusters</h1>
<ul>
<li>map: (k1, v1) -&gt; (k2, v2)</li>
<li>reduce: (k2, v2 list) -&gt; v2 list</li>
<li>Implementation
<ul>
<li>Single master, multiple mappers, multiple reducers</li>
<li>Split input and distribute to mappers</li>
<li>Mapper writes locally and informs master of write location</li>
<li>Reducers get location info from master and external sort data when ready</li>
<li>Reducers run reduce and create a file for each output</li>
</ul></li>
<li>Master stores status of each task and the position of the reducer data</li>
<li>If a worker dies (determined via a heartbeat), then the master re-runs the job and informs reducers to re-read data</li>
<li>More tasks means less skew but more memory pressure on the master</li>
<li>Backup tasks for stragglers</li>
<li>Refinements
<ul>
<li>Custom partition functions</li>
<li>Combiner functions</li>
<li>Skip records which cause failures</li>
<li>Local execution for debugging</li>
<li>Status info in HTML</li>
<li>Counters</li>
</ul></li>
</ul>
<h1 id="tag-a-tiny-aggregation-service-for-ad-hoc-sensor-networks">TAG: A tiny aggregation service for ad-hoc sensor networks</h1>
<ul>
<li>Motes running tiny OS form tree with increasing number broadcast</li>
<li>Periodic aggregate SQL queries</li>
<li>Aggregates expressed as initializer, merger, and final evaluator</li>
<li>Characterizing aggregates
<ul>
<li>Duplicate sensitivity</li>
<li>Exemplary vs summary</li>
<li>Increasing</li>
<li>Amount of state
<ul>
<li>Distributive: evaluator is identity</li>
<li>Algebraic: intermediate state is same size as identity</li>
<li>Holistic: proportional to input data size</li>
<li>Unique: proprtional to number of unique inputs</li>
<li>Context-senstive: proportional to statistical property</li>
</ul></li>
</ul></li>
<li>Motes have catalog of attributes, returning NULL if queries for unknown attribute</li>
<li>Reduce aggregates upwards</li>
<li>Build table of groups, evicting if need be</li>
<li>Prune increasing having queries early</li>
<li>Send downwards with deadlines</li>
<li>Optimizations
<ul>
<li>Snoop broadcast data and prune results early (e.g. hear about a bigger max)</li>
<li>Create data at top that can be pushed down to filter results (hypothesis testing)</li>
</ul></li>
<li>Nodes get new parent with best link quality when parent dies</li>
<li>2 optimizations for fault tolerance
<ul>
<li>Cache children values and re-send old ones</li>
<li>Split aggregate in two and send to two parents (e.g. COUNT/2)</li>
</ul></li>
</ul>
<h1 id="resilient-distributed-datasets-a-fault-tolerant-abstraction-for-in-memory-cluster-computing">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</h1>
<ul>
<li>Like MapReduce and Dryad but with caching</li>
<li>Like Pregel but more general purpose</li>
<li>Bulk data processing, unlike shared memory</li>
<li>Lineage for fast recovery</li>
<li>Narrow and wide dependencies</li>
<li>Schedule jobs with as many narrow deps as possible</li>
<li>Store intermediate data on mappers (like MR)</li>
<li>Re-run stuff when things fail</li>
<li>Interpreter magic</li>
<li>Evict partitions on an RDD level LRU; unless LRU is same RDD</li>
</ul>
<h1 id="combining-systems-and-databases-a-search-engine-retrospective"><a href="https://scholar.google.com/scholar?cluster=15869287167041695406">Combining Systems and Databases: A Search Engine Retrospective</a></h1>
<ul>
<li>Overview
<ul>
<li>This paper presents a retrospective on nine years of work on the Inktomi search engine and discusses how to architect a search engine using database principles.</li>
</ul></li>
<li>The Principles of Databases
<ul>
<li><strong>Top down design</strong>: design the semantics, then implement the mechanisms.</li>
<li><strong>Data independence</strong>: the independence of data representation and storage.</li>
<li><strong>Declarative query languages</strong>: declarative queries which are optimized.</li>
</ul></li>
<li>Overview
<ul>
<li>A crawler surfs the web, the indexer builds inverted indexes and scores documents, and the server parses, optimizes, and executes queries.</li>
<li>Queries consist of words (e.g.&#160;java, coffee, hamster) and properties (lang:english, contains:image) and return the documents with the given words that satisfy the given properties. Search results are scored based on document quality and the score of the words within the document (as determined by the indexer).</li>
</ul></li>
<li>Logical Query Plan
<ul>
<li>We store information in relations with one relation for the documents, one for the inverted index for words, and one for the inverted index for properties.</li>
<li>The query language is boolean expressions over words and filters with special care to only negate filters.</li>
</ul></li>
<li>Query Implementation
<ul>
<li>The query optimizer translates a logical query into a physical query with four operators (that I don&#8217;t fully understand).</li>
<li>The optimizer prefers flatter query plans with multiway pre-sorted merge joins on inverted indexes.</li>
<li>The query evaluator doesn&#8217;t pipeline and caches intermediate results for later.</li>
<li>The query optimizer goes top-down to help find the largest cached subexpression.</li>
<li>The inverted indexes and document relations are horizontally partitioned on the document id.</li>
<li>A load balancer sends a query to a master. The master optimizes the query and sends it to all the workers who find the top $k$ documents within their range of document ids. The master computes the global top $k$ and then looks up these top $k$ documents from the workers.</li>
<li>Nodes can compress the indexes (much like in a column store). Master can also send out top $k$ results to the suboordinates which they can use to prune their search space.</li>
</ul></li>
<li>Updates
<ul>
<li>The document and indexes relations are divided into chunks. The indexer and crawler create updates and insertions on the granularity of a chunk.</li>
<li>Every chunk is versions; versions are used for cache invalidation, debugging, rollback, etc. Upgrading to a new version involves atomically swapping to a new version.</li>
<li>We can also store a small deletion cache for real-time deletions of bad or illegal results.</li>
</ul></li>
<li>Fault tolerance
<ul>
<li>Fault tolerance is rather trivial for a search engine since it doesn&#8217;t store the master copy of the data and because availability is favored over anything else.</li>
<li>Popular chunks can be replicated; others don&#8217;t need to be.</li>
<li>Whenever a node fails, the query can be retried elsewhere.</li>
<li>A search engine can gracefully degrade under heavy load by only searching a subset of the chunks or by outright rejecting expensive queries.</li>
<li></li>
</ul></li>
<li>Other Topics
<ul>
<li><strong>Personalization</strong>: cookies can either store metadata or the id of a database entry which contains the metadata.</li>
<li><strong>Logging</strong>: search engines make a lot of logs and need custom database engines.</li>
<li><strong>Query rewriting</strong>: search engines can rewrite a query using context of current page.</li>
<li><strong>Phrase queries</strong>: e.g. &#8220;New York&#8221; search engine need nearness metrics.</li>
</ul></li>
</ul>
<h1 id="the-anatomy-of-a-large-scale-hypertextual-web-search-engine">The Anatomy of a Large-Scale Hypertextual Web Search Engine</h1>
<ul>
<li>Overview
<ul>
<li>In this paper, Larry and Sergey present the design of Google</li>
</ul></li>
<li>Design Goals
<ul>
<li>Improve search quality. Search engines at the time were <em>really</em> bad.</li>
<li>Provide a database of web pages and web searches to foster academic research on search engines.</li>
</ul></li>
<li>System Features
<ul>
<li><strong>PageRank</strong>: Given a site $A$ with incoming links from sites $T_1, \ldots, T_n$ and a damping factor $d$. The PageRank of the site satisfies the equation $PageRank(A) = (1 - d) + d\sum_{i=1}^n \frac{PageRank(T_i)}{OutDegree(T_i)}$.</li>
<li><strong>Anchor text</strong>: Google associates the text of a link with the site the link points to.</li>
<li><strong>Word location</strong>: Google records the location of every word found in a document to score documents based on where the words appear.</li>
<li><strong>Word font</strong>: Google records the size and boldness of a word in a document to score documents based on how important a word seems.</li>
</ul></li>
<li>Architecture
<ul>
<li>A <strong>URL server</strong> sends batches of URLs to a set of <strong>web crawlers</strong>. The web crawlers send web pages to a <strong>store server</strong> which compresses the web pages and puts them into a <strong>repository</strong>.</li>
<li>The <strong>indexer</strong> builds a forward index mapping documents to hits and stores it in a set of <strong>barrels</strong>. It also extracts all the anchors into a database. A <strong>URL resolver</strong> resolves the relative URLs to absolute URLs and generates a graph for PageRank. It also stores the anchor text of the pointed to documents in the forward index.</li>
<li>A <strong>sorter</strong> periodically inverts the index in place, changing the mapping from (document to words) to (word to documents).</li>
</ul></li>
<li>Data Structures
<ul>
<li><strong>BigFiles</strong>: really big files.</li>
<li><strong>Repository</strong>: a fat file of (doc id, length, url, compressed page) tuples.</li>
<li><strong>Document index</strong>: an ISAM index mapping doc ids to metadata, statistics, and pointers into the repository.</li>
<li><strong>Lexicon</strong>: a long list of words kept in memory.</li>
<li><strong>Forward index</strong>: the forward index is partitioned by word and stored sorted by doc id. Hit lists (lists of word occurrences in a file) is super compressed.</li>
<li><strong>Inverted index</strong>: there are two reverse indexes: one mapping to words to documents in which the word is important and another full inverted index.</li>
</ul></li>
<li>Crawling
<ul>
<li>The crawler was a Python program with 300 open connections, event loops, async IO, and local DNS cache. It was the most fragile part of whole operation.</li>
</ul></li>
<li>Indexing
<ul>
<li>Larry and Sergey had to write a custom HTML parser to handle exotic HTML and to run efficiently.</li>
<li>The system collects new words to add to lexicon and then adds them in batch later.</li>
</ul></li>
<li>Search
<ul>
<li>For single word queries, documents are ranked based on a lot of factors including the frequency of the words, the boldness of the words, PageRank, etc.</li>
<li>For multiword queries, the words are scored as before but additionally with the proximity of the words being taken into account.</li>
</ul></li>
</ul>
<h1 id="webtables-exploring-the-power-of-tables-on-the-web">WebTables: Exploring the Power of Tables on the Web</h1>
<ul>
<li>Scrape tables, filter bad one, create search engine + db admin tools</li>
<li>Scraping tables
<ul>
<li>Scrape 14.1 billion tables from Google</li>
<li>Filter bad ones using hand-written classifiers and classifiers trained on hand-labelled data</li>
<li>Prefer recall over precision (later pipeline will filter)</li>
<li>Another classifier to get schema</li>
<li>Build ACSDb schema -&gt; frequency</li>
</ul></li>
<li>Relation search
<ul>
<li>Just search google</li>
<li>Rank using classifier trained on features like hits in header, hits in first column, etc</li>
<li>Rank using classifier and a measure of the schema coherency using ACSDb</li>
<li>inverted index from word to table(x, y)</li>
</ul></li>
<li>ACSDb
<ul>
<li>Schema autocomplete: most likely attribute conditioned on existing attributes</li>
<li>Attribute synonym finding: likelihood of two synonyms appearing is low and conditioning on either word has similar effect</li>
<li>Join graph traversal: clusters neighbor of a node based on how similar they are</li>
</ul></li>
</ul>
<h1 id="materialized-views">Materialized Views</h1>
<ul>
<li>View maintenance
<ul>
<li>Homomorphic select</li>
<li>Distributive join</li>
<li>Derivation counting for poject (a drawback of the algebra approach)</li>
<li>Drawbacks of delta tables (top-k query and group by sum)</li>
<li>Summary-delta encoding of changes (e.g. total sales per item and store)</li>
<li>Optimizer should weigh updates against recomputation</li>
<li>We can maintain multiple views at once sometimes faster than all of them individually</li>
<li>Different views require different amounts of data (sometimes deltas, sometimes view, sometimes base data)</li>
<li>It's preferable to not depend on everything to maintain a view</li>
<li>Irrelevant updates (only look at deltas)</li>
<li>Self-maintainable view (look at view, but don't look at base data)</li>
<li>Runtime self-maintainable view (look at biew, but only look at base data sometimes)</li>
<li>We can materialize views as more than just tables (e.g. join views with pointers into base tables)</li>
<li>We can also maintain auxiliary views to speed up maintenace</li>
<li>Immediate vs deferred update</li>
<li>Imagine two transactions that update different rows in the same group; immediate update with write locks would have them conflict on the group (can use special locking protocols)</li>
</ul></li>
<li>Using views
<ol style="list-style-type: decimal">
<li>Can we answer a query with just views?</li>
<li>Can we get a maximal subset of a query with just views?</li>
<li>What is the fastest way to compute a query given views?</li>
</ol>
<ul>
<li>For a particular candidate query, expand the views and test for equivalence or maximal containement</li>
<li>Optimizer needs a view matching mechanism to know which views can be used for which queries</li>
<li>Data integration
<ul>
<li>Warehouse ETL style or mediated style over remote data</li>
<li>Local as-view (LAV) define local schemas as views over global schema and perform query rewrites to use views</li>
<li>Global as-view (GAB) define global schema s view over local schemas and perform view expansion</li>
</ul></li>
</ul></li>
<li>Selecting views to materialize
<ul>
<li>Define a performance metric (e.g. time for a given set of queries) and a resource limit (e.g. space for views or time for maintenace)</li>
<li>OLAP cube queries: materialize a subset of the cube</li>
<li>In general, the problem is very hard</li>
<li>Need to limit the search space of views based on what the optimizer can handle and in general to make search feasible</li>
</ul></li>
<li>Connections
<ul>
<li>Data stream processing</li>
<li>Approximate query processing</li>
<li>Scalable continuous query processing (see which updates trigger which views/triggers)</li>
<li>Caching</li>
<li>Indexes</li>
<li>Provenance (derivation counting is like provenance)</li>
</ul></li>
</ul>
<h1 id="on-the-computation-of-multidimensional-aggregates">On the Computation of Multidimensional Aggregates</h1>
<ul>
<li>Cubes, cuboids, and base cuboid</li>
<li>Homomorphic aggregation functions</li>
<li>Three methods for computation
<ul>
<li>Independent: compute all cuboids from base cuboid</li>
<li>Parent: iteratively compute every cuboid from its parent</li>
<li>Overlap: perform parent method in parallel</li>
</ul></li>
<li>Sorted run (matching l columns, l projected out)</li>
<li>Partition (matching l-1 columns, l projected out)</li>
<li>Choose parent with dropped attribute furthest right (e.g. R(A,C) wants R(A,C,D), instead of R(A,B,C))</li>
<li>If partitions fit in memory, we can compute in one pass and pipeline</li>
<li>Otherwise, we write sorted runs to disk and merge them later</li>
<li>Allocating which cuboids to run at once is NP-complete; breadth-first search left-to-right heuristic</li>
<li>Combine runs from later partitions into runs from earlier partitions</li>
<li>Make right cuboids small and sort attributes in decreasing order of distinct values to minimize the number of sorted runs</li>
</ul>
<h1 id="implementing-data-cubes-efficiently">Implementing Data Cubes Efficiently</h1>
<ul>
<li>Lattices, roll-ups, and drill-downs</li>
<li>Product lattice Hasse diagram as a hypercube</li>
<li>Cost model of evaluating cuboid is the minimum cardinality of cuboid greater than it</li>
<li>Minimize running all cuboids subject to fixed number of views: Repeatedly choose cuboid which maximizes benefit</li>
<li>Minimize running all weighted cuboids subject to fixed number of views: Repeatedly choose cuboid which maximizes weighted benefit</li>
<li>Minimize running all weighted cuboids subject to fixed space: Repeatedly choose cuboid which maximizes weighted benefit / space of cuboid</li>
<li>Benefit of greedy is at least 63% (1 - 1/e) of the optimal</li>
</ul>
<h1 id="informix-under-control-online-query-processing">Informix Under Control: Online Query Processing</h1>
<ul>
<li>OLAP data is big and queries are hard to write =&gt; waste a lot of time</li>
<li>Application scenarios
<ul>
<li>Online aggregation: see values for groups and toggle speed for each group</li>
<li>Online enumeration: lazy loaded spreadsheet</li>
<li>Online data visualization: a combination of aggregation and enumeration</li>
</ul></li>
<li>Random data access
<ul>
<li>Physically store data in order based on random pseudo-key</li>
<li>To insert, randomly replace existing tuple</li>
<li>To avoid non-random repeated scans, randomly offset scans or re-shuffle data every once in a while</li>
<li>Can also store a B+ tree on a random pseudo-key to keep the underlying data sorted in a more sane way</li>
</ul></li>
<li>Reorderability
<ul>
<li>If a user prefers to see more of one group of tuples, we have to select those quicker</li>
<li>Re-order operator can buffer tuples from below and preferentially output the ones the user wants, spilling the rest to disk</li>
<li>Can open up a pointer to every group in an index on the grouping columns and lottery schedule between them; works best with low-cardinality indexes to avoid a lot of random I/O</li>
</ul></li>
<li>Ripple joins
<ul>
<li>Cannot use blocking joins</li>
<li>Ripple joins allow us to change the rate of sampling of the two relations to better narrow down variance</li>
</ul></li>
<li>GROUP BY has to be implemented as a hash, not a sort</li>
<li>API
<ul>
<li>Direct API</li>
<li>OBDC embedding: UDFs for aggregates with confidence intervals, tuples returned with latest value, UDF for pausing or accelerating groups</li>
<li>Ideally, server could evaluate query while messages are being sent to client, but alternatively the server just outputs every k requests</li>
</ul></li>
<li>Implementation of operators
<ul>
<li>Ripple join re-scans same input multiple times, and we need to make sure that the scan order is the same every time</li>
<li>This can be hard when the operators (e.g. random scan or explicit re-order) may not return tuples in same order</li>
<li>Two options: cache and replay (spilling to disk if need be) and or make sure things below are determinstic</li>
</ul></li>
<li>Optimization
<ul>
<li>3 access plans (sequential scan, sequential scan with re-order, index trick) have varying degrees of speed and controllability</li>
<li>Re-order on GROUP BY from multiple joins future work</li>
<li>Optimization mostly future work</li>
<li>Join ordering: eddies</li>
</ul></li>
<li>ORDER BY and Having implemented by client</li>
</ul>
<h1 id="blinkdb-queries-with-bounded-errors-and-bounded-response-times-on-very-large-data">BlinkDB: Queries with Bounded Errors and Bounded Response Times on Very Large Data</h1>
<ul>
<li>Existing approaches to approximation
<ul>
<li>Predictable queries =&gt; super legit sketching</li>
<li>Predictable query predicates =&gt; good sampling</li>
<li>Predictable query column sets =&gt; BlinkDB</li>
<li>Unpredictable queries =&gt; online aggregation</li>
</ul></li>
<li>Built on HIVE</li>
<li>SELECT-FROM-WHERE-GROUPBY-HAVING queries without joins or subqueries</li>
<li>Confidence interval or latency as part of query</li>
<li>Sampling
<ul>
<li>Stratified sample n_max elements on query column sets</li>
<li>Store groups contiguously in blocks</li>
<li>Latency or accuracy determines n</li>
<li>n scales inversely proportional to root K</li>
<li>Choose appropriate cap K to get exactly n elements &lt; n_max</li>
</ul></li>
<li>Selecting samples
<ul>
<li>Given a space constraint, choose all samples with same cap K</li>
<li>Maximize product of probability of query, sparseness of query, and coverage</li>
<li>Coverage is the likelihood that a grouping column values appears in a sample. For example, given the sample on (a, b, c), we know every unique value of (a, b) will show up. Given a sample on a, some values of (a, b) might show up, some might not</li>
<li>If our sample isn't a perfect cover, we can still perform approximate queries, but our answers might be really bad</li>
</ul></li>
<li>Runtime
<ul>
<li>Choose smallest superset, or if no such set exists, run on subsamples and choose the one with the highest selectivity</li>
<li>Run on a tiny subsample to estimate selectivity and runtime and then use stats close forms to choose a right sized subsample</li>
<li>Store sampling rate to do things like COUNT</li>
</ul></li>
<li>Implementation
<ul>
<li>Compute uniform sample</li>
<li>Periodically re-compute which samples to use</li>
</ul></li>
</ul>
<h1 id="the-cql-continuous-query-language-semantic-foundations-and-query-execution">The CQL Continuous Query Language: Semantic Foundations and Query Execution</h1>
<ul>
<li><a href="https://mwhittaker.github.io/papers/html/arasu2006cql.html" class="uri">https://mwhittaker.github.io/papers/html/arasu2006cql.html</a></li>
<li>Overview
<ul>
<li>A database management system (DBMS) allows user to write ad-hoc queries against a static (or slowly changing) database. A data stream management system (DSMS) allows users to register stream queries which continuously run against streamed inputs.</li>
</ul></li>
<li>Streams and Relations
<ul>
<li>We assume an ordered time domain $\mathcal{T}$.</li>
<li>A <strong>stream</strong> $S$ is a multiset with entries of the form $(s, \tau)$ where $s$ is a tuple and $\tau \in \mathcal{T}$. We assume that there is a finite number of tuples per timestep.</li>
<li>A <strong>relation</strong> $R$ is a function from the time domain $\mathcal{T}$ to a multiset of tuples (i.e. to a standard relation). $R(\tau)$ is the relation at time $\tau$.</li>
</ul></li>
<li>Abstract Semantics
<ul>
<li>The abstract semantics is based on <strong>stream-to-relation</strong>, <strong>relation-to-relation</strong>, and <strong>relation-to-stream</strong> operators.</li>
</ul></li>
<li>Stream-to-relation
<ul>
<li><strong>Time-based windows</strong>: <code>S[Range T]</code> is a relation with all the tuples within the time range <code>T</code>.</li>
<li><strong>Tuple-based windows</strong>: <code>S[Rows n]</code> is a relation with the <code>n</code> most recent tuples; ties are broken arbitrarily.</li>
<li><strong>Partitioned windows:</strong> <code>S[Partition By A1, ..., Ak Rows n]</code> is a relation with the <code>n</code> most recent tuples for every group of <code>A1, ...,   Ak</code>.</li>
</ul></li>
<li>Relation-to-stream
<ul>
<li><code>Istream</code>$(R)$ at time $\tau$ contains $R(\tau) - R(\tau - 1)$. Used mostly with unbounded ranges.</li>
<li><code>Dstream</code>$(R)$ at time $\tau$ contains $R(\tau - 1) - R(\tau)$.</li>
<li><code>Rstream</code>$(R)$ at time $\tau$ contains $R(\tau)$. Used mostly with now ranges.</li>
<li>Istream and Dstream can be implemented using Rstream.</li>
<li>By default, unbounded ranges are added, and Istreams are added to monotonic queries.</li>
</ul></li>
<li>See paper for example queries.</li>
<li>Time Management
<ul>
<li>In order to process a query at time $\tau$, we have to know that there are no more inbound tuples with time $\leq \tau$. To do so, input sources use <strong>heartbeats</strong> (low watermark). A heartbeat at time $\tau$ indicates that no more inputs with time $\leq \tau$ will be sent. There are three mechanisms for heartbeats:</li>
<li>If timestamps are generated by a centralized DSMS, then calculating a low watermark is simple.</li>
<li>If input sources deliver tuples in increasing timestamp order, then we can collect heartbeats from the sources and use the minimum heartbeat as a low watermark.</li>
<li>If we have a global clock and bounded message delay, then we can infer the low watermark. For example, if it's 1:42 and the message delay is 1 minute, then we know that we have all messages from 1:41.</li>
</ul></li>
<li>Equivalences in CQL
<ul>
<li>All relation-to-relation query optimizations and materialized view optimizations can be leveraged.</li>
<li><strong>Window Reduction</strong>: <code>SELECT Istream(L) FROM S[Range Unbounded] WHERE C</code> is equivalent to <code>SELECT Rstream(L) FROM S[Now] WHERE C</code>.</li>
<li><strong>Filter-Window Commutativity</strong>: <code>(SELECT L FROM S WHERE C) [Range T]</code> is equivalent to <code>SELECT L FROM S[Range T] WHERE C</code>. Note the query is a simple SELECT-FROM-WHERE query.</li>
</ul></li>
<li>CQL Implementation in STREAM
<ul>
<li>Streams and relations are represented as a stream of triples $(s, \tau, \text{insert} | \text{delete})$ in nondecreasing timestamp order. Streams are represented as a sequence of insertions. Relations are represented as a sequence of insertions and deletions.</li>
<li>A query plan is a graph in which vertices are <strong>operators</strong> and edges are <strong>queues</strong> (in memory). Operators read streams and relations (in their unified format) from their input queues and output a stream or relation on their output queue.</li>
<li>Every operator has a corresponding <strong>synopsis</strong> (in memory) in which it can maintain its state. For example, a sliding window join may create a couple of hash tables.</li>
<li>Tuple are not copied when possible. Instead, they are stored in synopses, and tuple references are passed around.</li>
<li>Multiple queries can (and should) be combined into a single query plan.</li>
<li>A global scheduler decides how to evaluate the query plan graphs.</li>
<li>STREAM (a DSMS that implements CQL) has physical operators for all of the CQL operators and a couple of lower-level system operators.</li>
</ul></li>
<li>Questions
<ul>
<li>Q: Give an example stream query that has ambiguous semantics and explain how CQL makes the semantics clear.</li>
<li>A: ???</li>
<li>Q: What is the relationship between stream processing and materialized views?</li>
<li>A: The specification of a materialized view is a lot like a stream query. The materialized view is updated over time as the base tables change. Stream processing systems also use a lot of the same implementation strategies developed for materialized views.</li>
<li>Q: CQL includes both streams and relations. Can we remove relations and maintain the same expressiveness?</li>
<li>A: Yes. You can encode relations using streams. However, many concepts are more naturally expressed as time varying relations than as a stream of values.</li>
<li>Q: How can we implement Istream and Dstream as an Rstream?</li>
<li>A: ???</li>
<li>Q: Why is a SQL query plan a tree but a CQL query plan a graph?</li>
<li>A: ???</li>
<li>Q: How do you implement the stream-based relation-to-relation operators?</li>
<li>A: ???</li>
</ul></li>
</ul>
<h1 id="dataguides-enabling-query-formulation-and-optimization-in-semistructured-databases"><a href="https://scholar.google.com/scholar?cluster=1701940952301007499">Dataguides: Enabling Query Formulation and Optimization in Semistructured Databases</a></h1>
<ul>
<li>Object Exchange Model (OEM)
<ul>
<li>Possibly cyclic graph of objects (with a designated root node) in which each vertex has an object id and each edge is annotated with a label. Leaves are annotated with data.</li>
<li><strong>Label path</strong>: a sequence of labels $l_1.l_2 \ldots l_n$.</li>
<li><strong>Data path</strong>: a sequence of labels and oids $l_1.o_1 \ldots l_n.o_n$.</li>
<li><strong>Instance of data path</strong>: $d$ is an instance of $l$ if the labels match.</li>
<li><strong>Target set of label path</strong>: set of oids reachable from $l$.</li>
<li>The Lorel query language allows querying OEM objects. For example, the query <code>SELECT Restaurant.Name WHERE Restaurant.Entree = 'Burger';</code>.</li>
</ul></li>
<li>DataGuides
<ul>
<li>A <strong>DataGuide</strong> for an OEM object $s$ is an OEM object $d$ such that every label path of $s$ has exactly one data path in $d$ $d$, and every label path of $d$ $d$ is a label path of $s$ $s$.</li>
<li>A DataGuide $d$ can be used to quickly determine which label paths exist in $s$. It can also be used to determine which labels exist after a label path $l$.</li>
<li>Creating a DataGuide $d$ of $s$ is equivalent to determinizing $s$.</li>
<li>Multiple DataGuide exist, and the minimal one is not always preferable. It's harder to maintain and precludes annotations (described next).</li>
<li>A strong DataGuide $d$ of $s$ is a DataGuide with a bijection between the target sets of $s$ and the objects of $d$. It allows us to unambiguously store annotations in the objects of $d$.</li>
<li>To create a strong DataGuide, we simply perform the vanilla determinization procedure. Determinization can take exponential time, but the paper presents some experimental results showing that for typical databases, determinization runs quick enough.</li>
</ul></li>
<li>Incremental maintenance
<ul>
<li>We represent changes to an OEM object as a set of edge insertions and deletions where each edge is represented as $u.l.v$.</li>
<li>For each $u.l.v$, we re-run the determinization procedure at every vertex that contains $u$, avoiding re-doing work that's already been done.</li>
</ul></li>
<li>Query Formulation
<ul>
<li>Lore provides a web UI that users can use to browse DataGuides. They can expand and collapse the DataGuide and see samples of data for each object.</li>
<li>Users can also provide simple filters to form Lorel queries.</li>
</ul></li>
<li>Query Optimization
<ul>
<li>DataGuides expectedly speed up queries for a specific label path. They reduce the cost to time linear in the length of the label path.</li>
<li>Lore also has value indexes which can say, for example, the set of oids for a objects with incoming label year and value less than 1965. We can intersect DataGuide oids and vindex oids to answer SELECT-WHERE queries.</li>
</ul></li>
</ul>
<h1 id="powergraph-distributed-graph-parallel-computation-on-natural-graphs">PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs</h1>
<ul>
<li>Introduction
<ul>
<li>&quot;Graph-parallel abstractions rely on each vertex having a small neighborhood to maximize parallelism and effective partitioning to minimize communication&quot;.</li>
<li>Existing graph processing frameworks do not handle power-law graphs (graphs in which a small number of vertices are incident to a large fraction of the edges) well.</li>
</ul></li>
<li>Graph-Parallel Abstractions
<ul>
<li>&quot;In contrast to more general message passing models, graphparallel abstractions constrain the interaction of vertexprogram to a graph structure enabling the optimization of data-layout and communication.&quot;</li>
<li><strong>Pregel</strong>. In a series of super-steps, a vertex with sum its inputs using an associative commutative operator, perform a vertex function, and then send messages to all its neighbors. Pregel terminates when there are no pending messages and all vertices vote to terminate. Note that Pregel is a bulk synchronous message passing abstraction.</li>
<li><strong>GraphLab</strong>. Data is stored on each vertex and on each edge. A vertex can read the data on all neighboring edges and vertices and then update its state. Vertices can also schedule neighbors to run. GraphLab ensures serializability. Note that GraphLab is an asynchronous shared memory abstraction.</li>
<li>In the gather-apply-scatter abstraction (GAS), vertices sum gathered data from neighbors ($S \gets \oplus_{u} g(D_v, D_{v,u}, D_u)$), apply the sum to make a new state ($D_v' \gets a(D_v, S)$), and scatter the state to all neighbors ($D_{v,u}' \gets s(D_v', D_{v,u}, D_u)$).</li>
</ul></li>
<li>Challenges of Natural Graphs
<ul>
<li>Existing graph processing frameworks require computation, communication, and storage proportional to the degree of the vertex, which can be highly skewed. Power-law graphs can also be very hard to partition.</li>
</ul></li>
<li>PowerGraph Abstraction
<ul>
<li>&quot;By lifting the Gather and Scatter phases into the abstraction, PowerGraph is able to retain the natural &#8220;think-like-a-vertex&#8221; philosophy [30] while distributing the computation of a single vertex-program over the entire cluster.&quot; Pregel abstracts gather but not scatter. GraphLab doesn't abstract either.</li>
<li>PowerGraph vertex programs explicitly implement <code>gather</code>, <code>sum</code>, <code>apply</code>, and <code>scatter</code> functions which are parallelized over edges. Vertex programs also explicitly activate neighbors.</li>
<li>If the accumulator forms an Abelian group, then scatter can return deltas which are added to neighboring state to maintain the summed gather incrementally.</li>
<li>PowerGraph programs can be executed in a bulk synchronous fashion (gather/sum is called on all nodes, then apply, then scatter) or in an asynchronous (yet serializable) fashion.</li>
<li>PowerGraph subsumes Pregel and GraphLab.</li>
</ul></li>
<li>Distributed Graph Placement
<ul>
<li>Most graph processing systems perform an edge cut. Vertices are distributed as evenly as possible while minimizing the number of cut edges. Existing graph processing frameworks do not partition power-law graphs well.</li>
<li>With PowerGraph, vertices (as opposed to edges) are cut. Gather and scatter is applied in parallel while intermediate accumulators are communicated between vertex replicas. The distribution strategy tries to minimize the average amount of vertex duplication under the constraint that the edges are evenly divided (modulo a slack factor) amongst the machines.</li>
<li>A random vertex cut (where we randomly assign edges to machines) turns out to a pretty good vertex cut for power-law graphs. This also ends up being pretty good for non-power-law graphs.</li>
<li>A greedy heuristic can perform even better. Sequentially assign each edge $(u, v)$. If $u$ and $v$ reside on the same node (i.e. $A(u) \cap A(v) \neq \emptyset$), put the edge there. If $u$ and $v$ are both assigned but do not reside on the same node (i.e. $A(u) \neq \emptyset, A(v) \neq \emptyset, A(u) \cap A(v) = \emptyset$), then put the edge on a machine for $u$ or $v$, whichever has more unassigned edges. If only one of $u$ or $v$ has been assigned, then put the edge there. If neither $u$ nor $v$ has been assigned, then put the edge on the least loaded machine.</li>
<li>This algorithm is guaranteed to be better than the random algorithm, but it requires more coordination.</li>
</ul></li>
<li>Implementation
<ul>
<li>Serializability is achieved using a fancy dining philosophers algorithm.</li>
<li>Fault tolerance is implemented with snapshot and fancy distributed snapshot algorithms.</li>
</ul></li>
<li>Questions
<ul>
<li>Q: Describe the implementation of the greedy partition algorithm in detail. How is the work parallelized? What state has to be coordinated?</li>
<li>A: ???</li>
</ul></li>
</ul>
<h1 id="schema-mapping-as-query-discovery">Schema Mapping as Query Discovery</h1>
<ul>
<li>Overview
<ul>
<li>We have a source database and a target database with a different schema. The user specifies how to map tuples from the source to the target, and the systems figures out a SQL query to do the mapping.</li>
</ul></li>
<li>Value Correspondences
<ul>
<li>Existing work on schema mapping involved set-based <strong>schema assertions</strong>. This paper proposes <strong>value correspondences</strong> which instead involve showing how pairs of tuples map to another tuple. This is arguably easier to understand and can lead to automatically finding a query to perform the mapping.</li>
<li>At a high level, a value correspondence is (1) a function mapping a tuple of source tuples to an output tuple and (2) a filter on the source tuples.</li>
<li>For example, a user might specify that <code>PayRate(HourlyRate) * WorksOn(Hours)</code> equals <code>Personnel(Salary)</code>, and it is the job of the system to find a query which decides which tuples from <code>PayRate</code> and which tuples from <code>WorksOn</code> to join.</li>
<li>Principled heuristics:
<ul>
<li>Every source tuple should contributed to at most one output tuple. This implies to use joins instead of cross products.</li>
<li>Every source tuple should contribute to at least one output tuple. This implies to use unions over intersections and outer joins over inner joins.</li>
</ul></li>
</ul></li>
<li>Query Discovery Algorithm
<ul>
<li>Formally, a value correspondence is a pair $(f, p)$ of a mapping function $f$ and predicate $p$. $f$ maps values drawn from the domain of source attributes to a single value from the domain of a target attribute. $p$ is a boolean function on some (possible different) set of source attributes. Both functions can also take in attribute metadata ( e.g.&#160;attribute name, relation name, low and high value, etc.)</li>
<li>Consider source relations $R(a, b)$ and $S(c, d)$ and target relation $T(x, y)$. Say we have correspondences $v_1: (R.a, R.b) \to T.x$, $v_2: S.c \to T.y$, and $v_3: R.b, S.c \to T.x$ The algorithm proceeds in four steps.
<ul>
<li>First, we group correspondences into <strong>candidate sets</strong> where each candidate set maps to each output attribute at most once. In our example, this is $\{\set{v_1, v_2}$, $\set{v_2, v_3}$, $\set{v_1}$, $\set{v_2}$, $\set{v_3}$, $\emptyset{}\}$. If a candidate set maps to <em>every</em> output attribute, then we call it a <strong>complete candidate set</strong>.</li>
<li>Second, we try to find a good join order for all the candidate sets that read from multiple relations. We try to infer join orders using foreign keys, asking the user for help otherwise. If no join order is found, we discard the candidate set. This might leave us with $\{\set{v_1, v_2}$, $\set{v_1}$, $\set{v_2}$, $\set{v_3}$, $\emptyset{}\}$.</li>
<li>Third, we find a subset (called a <strong>cover</strong>) of all the candidate sets that includes all value correspondences. We prefer smaller covers and covers which produce fewer null outputs. For example, we might select the cover $\set{\set{v_1, v_2}, \set{v_3}}$.</li>
<li>Fourth, we generate a query for every candidate set in the best cover and union them all together. Value correspondences go in the <code>SELECT</code> and filters go in the <code>WHERE</code>.</li>
</ul></li>
</ul></li>
<li>Making the Algorithm Incremental
<ul>
<li>Consider we have a cover $\{\set{v_1, v_2}$, $\set{v_3}$, $\set{v_4, v_5}\}$. If we get a new value correspondence $+v_6$, then we try to insert $v_6$ into every candidate set in the cover. This will give us three alternative covers. These new covers are given to the user to rank.</li>
</ul></li>
<li>Nested-Sets in Target Relations
<ul>
<li>Clio can also target nested relational output schemas by compiling to nested SQL queries.</li>
</ul></li>
</ul>
<h1 id="provenance-in-databases-why-how-and-where"><a href="https://scholar.google.com/scholar?cluster=14688264622623487965">Provenance in Databases: Why, How, and Where</a></h1>
<ul>
<li>Overview
<ul>
<li>There is lineage, why-provenance, how-provenance, and where-provenance.</li>
<li>There are two approaches to computing provenance: the <strong>eager</strong> approach and the <strong>lazy</strong> approach. In the eager approach, queries are transformed to compute some bookkeeping metadata which is stored alongside the output data. In the lazy approach, the query, input, and output are inspected to infer the provenance.</li>
</ul></li>
<li>Lineage
<ul>
<li>The <strong>lineage</strong> of a tuple $t \in Op(R_1, \ldots, R_n)$ is a sequence $(R_1&#8217;, \ldots, R_n&#8217;)$ where $R_i&#8217; \subseteq R_i$ that satisfies three properties:
<ol type="1">
<li>$Op(R_1&#8217;, \ldots, R_n&#8217;) = \set{t}$. (&#8220;lineage is relevant&#8221;)</li>
<li>For every $R_i&#8217;$ and for every $u \in R_i&#8217;$, $Op(R_1&#8217;, \ldots, \set{u}, \ldots R_n&#8217;) \neq \emptyset{}$. (&#8220;lineage doesn&#8217;t contain irrelevant facts&#8221;)</li>
<li>The lineage is maximal. (&#8220;lineage is complete&#8221;)</li>
</ol></li>
<li>The original work on lineage provides lineage definitions for select, project, join, union, difference, and group by. Self-joins are not supported, and lineage is only defined for tuples that appear in the output. The original work also had a definition for the lineage of a composition of operators.</li>
</ul></li>
<li>Compositional Definition of Lineage
<ul>
<li>$\textsf{Lin}(Q, I, t)$ returns a subset of $I$.</li>
<li>A tuple has empty lineage (i.e. $\emptyset$) if it was constructed from the query itself and has no lineage (i.e. $\bot$) if it does not appear in the query output at all.</li>
<li>See paper for $\textsf{Lin}$ definition.</li>
<li>Lineage is invariant to query rewrite assuming there are no repeated relations.</li>
</ul></li>
<li>WHIPS
<ul>
<li>WHIPS implements lineage lazily.</li>
<li><p>WHIPS converts SPJ queries into PSJ normal form and generates a reverse query to extract the lineage. For example, the lineage of the tuple <code>(1, 2)</code> from the query</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode sql"><code class="sourceCode sql"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">SELECT</span> R.a, S.c</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">FROM</span> R, S</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="kw">WHERE</span> R.b = S.b <span class="kw">AND</span> S.d = <span class="dv">12</span></a></code></pre></div>
<p>can be computed with the query</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode sql"><code class="sourceCode sql"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">SELECT</span> *</a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="kw">FROM</span> R, S</a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="kw">WHERE</span> R.b = S.b <span class="kw">AND</span> S.d = <span class="dv">12</span> <span class="kw">AND</span> R.a = <span class="dv">1</span> <span class="kw">AND</span> S.c = <span class="dv">2</span></a></code></pre></div></li>
<li>WHIPS can also handle generic SPJUAD queries by compiling a query into multiple reverse queries. At a high level, the query is divided into AUSPJ segments, and each segment is materialized. Then, reverse queries are run backwards through the segments.</li>
</ul></li>
<li>Why-provenance
<ul>
<li>A witness of an output $t$ with respect to query $Q$ is a subset $I&#8217; \subseteq I$ such that $t \in Q(I&#8217;)$.</li>
<li>The why-provenance of a tuple is a subset of the <strong>witness basis</strong> (the set of all witnesses of the tuple).</li>
<li>See paper for why-provenance rules.</li>
<li>An element of the why-provenance is called a <strong>proof-witness</strong>. Every witness is a superset of a proof-witness.</li>
<li>Minimal witness basis (the minimum elements of the witness basis) and minimal why-provenance (the minimum elements of the why provenance) are the same and are invariant to query rewrites.</li>
</ul></li>
<li>View Deletion Problem
<ul>
<li>View deletion is closely related to why-provenance. If we want to remove a tuple from a view, we have to remove a tuple from every proof-witness.</li>
<li><strong>View side-effect problem</strong>: Given a source database $I$, a query $Q$, the view $V = Q(I)$, and a tuple $t \in V$, find a subset $\Delta I \subset I$ whose removal will delete $t$ from $V$ while minimizing the number of other tuples deleted from the view.</li>
<li><strong>Source side-effect problem</strong>: Given a source database $I$, a query $Q$, the view $V = Q(I)$, and a tuple $t \in V$, find the smallest subset $\Delta I \subset I$ whose removal will delete $t$ from $V$.</li>
</ul></li>
<li>How-provenance
<ul>
<li>A commutative semiring is a commutative ring with identity in which there may not be additive inverses.</li>
<li>A $K$-relation is a function mapping tuples to elements of the commutative semiring $K$.</li>
<li>See paper for rules on evaluating queries over $K$-relations.</li>
<li>The how-provenance of a tuple $t$ with respect to a query $Q$ is $Q(I)(t)$ where $I$ is interpreted as a $\mathbb{N}[TupleLoc]$-relation.</li>
</ul></li>
<li>Trio
<ul>
<li>Trio computes the lineage of select, project, join, union, intersect, difference, duplicate elimination, and group by as polynomials. When looking only at the basic select, project, join, union, it is exactly like how-provenance.</li>
<li>Given the provenance of a tuple, we can compute the likelihood of its existence. For example, if the lineage of a tuple is $t_1 (t_2 + t_3)$, then the its likelihood is $Pr(t_1 \land (t_2 \lor t_3))$.</li>
</ul></li>
<li>Provenance Semirings and Recursion
<ul>
<li>Green et al.&#160;extended $K$-relation queries to datalog.</li>
<li>Now, how-provenance are formal power series $\mathbb{N}^\infty[[TupleLoc]]$. These can be thought of as a product of a sum of all proof tree leaves or as a solution to a recursive set of equations.</li>
</ul></li>
<li>Schema Mappings
<ul>
<li>Schema mappings
<ul>
<li>A schema mapping maps a source schema to a target schema through a set of source-to-target dependencies (e.g. $\text{Agencies}(n,b,p)$ $\implies$ $\exists I. \text{Tripts}(I, n, p)$) and target dependencies (e.g. $\text{Transportation}(i, t, p)$ $\implies$ $\exists N. \exists P. Trips(i, N, P).$).</li>
</ul></li>
<li>ORCHESTRA
<ul>
<li>Given a source instance, a target instance, and a schema mapping that connects the two, ORCHESTRA will derive target tuples and put them into the target database whenever tuples are inserted in the source database. The provenance of these target tuples is stored eagerly.</li>
<li>The provenance is used for two things: trustworthiness filtering and incremental deletion.</li>
<li><strong>Trustworthiness filtering</strong>: If a source tuple or mapping is deemed untrustworthy, then all tuples derived from it (checked using provenance) are excluded from the target database.</li>
<li><strong>Incremental deletion</strong>: When tuples are deleted from the source database, provenance is used to determine which tuples in the target database need to be deleted.</li>
</ul></li>
<li>Routes and SPIDER
<ul>
<li>Spider has routes which show how tuples are derived from one another. This is useful for debugging.</li>
<li>There&#8217;s a bunch of other super complicated stuff too; see paper for details.</li>
</ul></li>
</ul></li>
</ul>
<h1 id="wrangler-interactive-visual-specification-of-data-transformation-scripts"><a href="https://scholar.google.com/scholar?cluster=4069328732173546194">Wrangler: Interactive Visual Specification of Data Transformation Scripts</a></h1>
<ul>
<li>Overview
<ul>
<li>Data scientists spend an inordinate amount of their time wrangling data. Wrangler makes data wrangling much easier.</li>
</ul></li>
<li>Example workflow
<ul>
<li>See the paper for an example workflow in which the analyst loads a CSV file, deletes empty rows, extracts the state name from some text, fills in missing entries in the state column, deletes some more rows, and then generates a pivot table.</li>
<li>The Wrangler UI consists of a table of data on the right, transforms and suggestions on left, and a data quality meter on the top above each column.</li>
</ul></li>
<li>Wrangler Transformation Language
<ul>
<li>Wrangler specifies transforms as a declarative transformation language.</li>
<li><strong>Map</strong> transforms map one row to zero (filter), one (map), or many (split) rows.</li>
<li><strong>Lookups and joins</strong> can be used to integrate external data like mappings from zip code to state or correcting typos.</li>
<li><strong>Reshape</strong> transforms reorganizes the shape of the table, similar to pivot tables.</li>
<li><strong>Positional</strong> transforms fill data from neighboring data or shift data around within the table.</li>
<li>Wrangler also has transforms for <strong>sorting</strong>, <strong>aggregation</strong>, etc.</li>
<li>Wrangler also has the notion of <strong>data types</strong> (e.g.&#160;int, string) and <strong>semantic roles</strong> (e.g.&#160;zip code).</li>
</ul></li>
<li>Wrangler Interface Design
<ul>
<li>Direct manipulation and visual selections.</li>
<li>Automatic suggestion with an inference engine.</li>
<li>Menu-based transform selection.</li>
<li>Natural language descriptions and in-place visual transform previews.</li>
<li>Manual editing of transform parameters.</li>
<li>Export history to script and change history of script.</li>
<li>Users can place comments on the transformations.</li>
<li>Data quality bar with green (satisfies data type and semantic role), yellow (satisfies data type but not semantic role), red (doesn&#8217;t satisfy data type), and gray (missing).</li>
</ul></li>
<li>Inference algorithm
<ul>
<li>There are four types of parameters: row selections, column selections, text selections and enumerables.</li>
<li>The inference algorithm proceeds in the following phases: infer parameters, generate a list of transforms, rank the transforms.</li>
<li>Inferring parameters involves looking at what&#8217;s clicked and inferring some regular expressions.</li>
<li>Enumerating transforms is simple. It tries every transform on every parameter.</li>
<li>The ranking engine prefers simple transform types (e.g.&#160;column) over other more complicated ones (e.g.&#160;row selection). Within a type, it filters based on frequency within a corpus of user interactions. It also ensures a diversity of different operations are suggested.</li>
</ul></li>
<li>Questions
<ul>
<li>Q: What makes Wrangler&#8217;s transformation language declarative?</li>
<li>A: ???</li>
</ul></li>
</ul>
<h1 id="the-madlib-analytics-library-or-mad-skills-the-sql">The MADlib analytics library: or MAD skills, the SQL</h1>
<ul>
<li>Machine learning in a database</li>
<li>Why database? Data already there, lots of know-how</li>
<li>Why not database? Mismatch between ML workloads and SQL</li>
<li>Macro-programming (partitioning and moving data)
<ul>
<li>UDFs as transitions function, merge function, and final function</li>
<li>UDFs don't do multiple iterations; python drivers with temp table intermediate state</li>
<li>UDFs are not polymorphic; Python code which generates UDFs</li>
</ul></li>
<li>Microprogramming
<ul>
<li>Fast linear algebra library for dense linear algebra</li>
<li>Custom library for sparse linear algebra</li>
<li>C++ abstraction for writing lower-level code</li>
</ul></li>
<li>Examples: least squares regression, logistic regression, k-means clustering</li>
<li>Wisconsin stochastic gradient descent, Berkeley and Florida text analytics</li>
</ul>
<h1 id="hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent">HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent</h1>
<ul>
<li>Sparse cost functions: each step of gradient descent only updates a small fraction of the weight vector (e.g. sparse SVMs)</li>
<li>Metrics of sparsity
<ul>
<li>maximum size of index set</li>
<li>maximum fraction of nodes that contain a common node</li>
<li>maximum fraction of nodes that overlap a common index set</li>
</ul></li>
<li>Algorithm:
<ul>
<li>Randomly sample a vector</li>
<li>Compute gradient</li>
<li>Update appropriate entries</li>
</ul></li>
<li>Convergence is based on the sparsity metrics</li>
</ul>
<h1 id="scaling-distributed-machine-learning-with-the-parameter-server">Scaling Distributed Machine Learning with the Parameter Server</h1>
<ul>
<li>A key-value store that stores (i, wi) pairs for a vector w</li>
<li>Can use e.g. to run batch gradient descent in parallel</li>
<li>Cluster of servers and clusters of workers</li>
<li>Features
<ul>
<li>Range updates</li>
<li>User defined functions to run on server</li>
<li>Asynchronous task execution</li>
<li>Serial, eventual, and bounded delay consistency</li>
</ul></li>
<li>Consistent hashing (with single master) and chain replication</li>
<li>Compress messages</li>
</ul>
  </div>

  <script type="text/javascript" src="../js/mathjax_config.js"></script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</body>
</html>
