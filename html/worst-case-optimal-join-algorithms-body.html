<h1 id="worst-case-optimal-join-algorithms">Worst-Case Optimal Join Algorithms</h1>
<ul>
<li>tl;dr
<ul>
<li>Say a conjunctive query can output at most (|R||S||T|)^3/2 tuples. We want a join algorithm that will <em>always</em> run in O(|R||S||T|^3/2) time.</li>
</ul></li>
<li>Given a natural join query like <code>R(A, B), S(B, C), T(C, A)</code>, we construct a hypergraph as follows:
<ul>
<li>Each attribute (e.g. <code>A</code>, <code>B</code>, <code>C</code>) becomes a vertex.</li>
<li>Each relation becomes a hyperedge (i.e.&#160;a subset of the vertices) of its attributes. For example, <code>R(A, B)</code> becomes the hyperedge <code>{A, B}</code>.</li>
</ul></li>
<li>Next, assign a non-negative weight to each edge. Go to each vertex and sum up the weights assigned to the edges that contain it. Make sure that this sum is greater than equal to 1 for every vertex.</li>
<li>The number of tuples in the output of this query is guaranteed to be less than |R|^{weight of R} |S|^{weight of S} |T|^{weight of T} for any such edge cover.</li>
<li>A worst case optimal join algorithm will always run in O(worst case size of q
<ul>
<li>the sum of the sizes of all the input relations).</li>
</ul></li>
<li>The actual algorithm is short, though it&#8217;s crazy recursive and hard to understand. Leapfrog join is an actual worst case optimal join that people have implemented. The algorithms require some initial indexing as well.</li>
<li>In practice, worst case optimal isn&#8217;t always what we&#8217;re aiming for. Also, more standard join algorithms might be easier to make fast (e.g.&#160;cache locality, prefetching, distribution, etc.)</li>
</ul>
